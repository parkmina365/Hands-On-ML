{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2. 신경망과 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10장. 케라스를 사용한 인공 신경망 소개\n",
    "- 인공신경망(ANN, Artificial Neural Network): 뇌에 있는 생물학적 뉴런(신경 세포)의 네트워크에서 영감을 받은 머신러닝 모델. 인공 신경망은 뉴런에서 아이디어를 얻었을 뿐 생물학적 뉴런과는 멀어지고 있음. 데이터에서 패턴을 점진적으로 학습하는 수학 모델에 가까움\n",
    "- 인공신경망은 딥러닝의 핵심. 이미지 분류, 음성인식, 추천 시스템, 강화학습 등 머신러닝 문제를 다루는데 적합함\n",
    "- 인공 신경망의 초창기 구조 -> 다층 퍼셉트론 -> 케라스 API 사용하여 인공 신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 생물학적 뉴런에서 인공 뉴런까지\n",
    "- 인공 신경망의 역사는 꽤 긴편. 과거 SVM등 머신러닝 기술들이 인공 신경망보다 더 나은 결과를 보여 침체기를 보였으나 현재는 인공 신경망의 새로운 부흥기\n",
    "    - 신경망 훈련을 위한 데이터 증대. 인공 신경망은 규모가 크고 복잡한 문제에서 다른 머신러닝보다 좋은 성능을 냄\n",
    "    - 컴퓨터 하드웨어의 발전. 신경망 훈련 학습 시간 감소. 클라우드 플랫폼 발전\n",
    "    - 훈련 알고리즘 향상\n",
    "    - 인공 신경망의 이론상 제한이 실전에서는 문제가 되지 않음. 예: 지역 최저점에 갇혀 인공 신경망 훈련 알고리즘이 해결책을 찾지 못할 것으로 생각했으나 실제는 굉장히 드문일(지역 최저점에 도덜하더라도 일반적으론 전역 최적점에 매우 가까움)\n",
    "    - 투자와 진보의 선순환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.1 생물학적 뉴런(BNN, Biological Neural Network)\n",
    "- 구조: 수상돌기, 축삭돌기, 시냅스(말단), 활동 전위(AP, Action potential) 또는 신호, 신경 전달물질\n",
    "- 뉴런은 다른 수많은 뉴런 네트워크와 연결. 복잡한 계산 수행 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.2 뉴런을 사용한 논리 연산\n",
    "- 생물학적 뉴런에서 착안한 단순한 신경망 모델. 나중에 인공 뉴런이 됨\n",
    "    - 이진 입력, 이진 출력 하나를 가짐. 단순히 입력이 일정 개수만큼 활성화 되었을때 출력을 내보낸다 가정한 모델\n",
    "- 이런 간단한 모델로 인공뉴런의 네트워크를 만들어 어떤 논리 명제도 계산할 수 있다는 것을 증명함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.3 퍼셉트론(Perceptron)\n",
    "- 가장 간단한 인공 신경망 구조 중 하나\n",
    "- TLU(Threshold Logic Unit) 또는 LTU(Linear Threshold Unit)이라 불리는 조금 다른 형태의 인공 뉴런을 기반\n",
    "    - 입력과 출력이 on/off 개념의 이진값이 아닌 어떤 숫자이며, 각각의 입력 연결은 가중치와 연관되어 있음\n",
    "    - 입력에 가중치를 반영한 합을 계산한 후, 그 합에 계단 함수(Step function)를 적용하여 결과 출력\n",
    "    - 계단 함수: 임계 값을 기준으로 binary 값을 출력하는 함수. 일반적으로 헤비사이드 계산 함수를 사용\\\n",
    "    부호 함수(임계 값에도 출력값을 부여하여 출력 값이 3가지)를 대신 사용하기도 함\n",
    "- 하나의 TLU는 간단한 선형 이진 분류 문제에 사용 가능. TLU를 훈련한다는 것은 최적의 가중치를 찾는 것\n",
    "\n",
    "- 퍼셉트론은 층이 하나뿐인 TLU로 구성됨. 각 TLU는 모든 입력에 연결되어 있음\n",
    "- 완전 연결층 = 밀집층: 한 층에 있는 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있는 경우\n",
    "- 퍼셉트론의 입력은 입력 뉴런이라고 불리는 특별한 통과 뉴런에 주입됨. 어떤 입력이 주입되든 출력으로 통과시킴. 입력층은 모두 입력 뉴런으로 구성. 거기에 편향 이라는 특성이 더해짐. 편향 특성은 항상 1을 출력하는 특별한 종류의 뉴런인 편향 뉴런(bias neuron)으로 표현됨\n",
    "- sklearn.linear_model은 하나의 TLU 네트워크를 구현한 Perceptron 클래스를 제공\n",
    "  - Perceptron 클래스: 매개변수 loss='perceptron', learning_rate='constant', eta0=1(학습률), penalty=None(규제없음)인 SGDClassifier와 같음\n",
    "  - 로지스틱회귀와는 달리 퍼셉트론은 클래스 확률을 제공하지 않음. 고정된 임계값을 기준으로 예측을 만듦(로지스틱 회귀가 더 선호됨)\n",
    "- 퍼셉트론의 약점: XOR(배타적 논리합)분류문제 등은 풀지 못함. 이는 다른 선형 분류모델도 마찬가지\n",
    "- 다층 퍼셉트론(MLP): 퍼셉트론을 여러개 쌓아올린 형태로 XOR 문제를 풀 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.4 다층 퍼셉트론(MLP)과 역전파(Backpropagation)\n",
    "- 다층 퍼셉트론: 입력층, 은닉층(TLU층으로 하나 이상), 출력층으로 구성\n",
    "  - 입력에 가까운 층: 하위층, 출력에 가까운 층: 상위층\n",
    "  - 출력층을 제외하면 모든 층은 편향 뉴런을 포함. 다음 층과 완전 연결되어 있음\n",
    "  - FNN(Feedforward Neural Network): 신호가 입력 -> 출력의 한 방향으로만 흐르는 구조\n",
    "- DNN(Deep Neural Network): 은닉층을 여러개 쌓아올린 인공 신경망\n",
    "- 역전파(Backpropagation): 다층 퍼셉트론을 훈련하는 알고리즘. 그래디언트를 자동으로 계산하는 경사 하강법. 네트워크를 두번(정방향, 역방향) 통과하는 것만으로 모든 모델 파라미터에 대한 네트워크의 오차의 그래디언트를 계산할 수 있음. 오차를 감소시키기 위해 각 연결 가중치와 편향값이 어떻게 바뀌어야 할지 알 수 있음\n",
    "  1. 정방향 계산: 각 훈련 샘플에 대해 역전파 알고리즘이 먼저 예측을 만들고 각 층의 오차를 측정함(손실함수 사용)\n",
    "  2. 역방향 계산: 역방향으로 각 층을 거치며 각 층의 출력 연결이 오차에 기여하는 정도를 측정\n",
    "  3. 경사 하강법: 경사 하강법을 수행하여 이 오차가 감소하도록 가중치를 조정\n",
    "- 활성화 함수의 변화: 역전파 알고리즘을 잘 작동시키기 위해 계단 함수를 시그모이드, tanh, Relu로 변화\n",
    "  - 시그모이드 함수: -1, 0 범위의 S자 모양의 함수\n",
    "  - tanh(하이퍼볼릭 탄젠트)함수: -1, 1 범위의 S자 모양의 함수\n",
    "  - Relu 함수: max(0,x): 계산 속도가 빠르다는 장점. 출력에 최대값이 없으므로 시그모이드 함수, tanh 함수에서 발생하는 경사 하강법의 일부 문제를 완화해줌\n",
    "- 활성화 함수를 사용하는 이유: 선형 변환을 여러 개 연결해도 얻을 수 있는것은 선형 변환뿐. 층 사이에 비선형성을 추가해야 함. 비선형 함수가 있는 충분히 큰 심층 신경망은 연속 함수에 근사 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.5 회귀를 위한 다층 퍼셉트론\n",
    "- 회귀의 경우 출력 뉴런이 하나만 필요함\n",
    "    - 다변량 회귀(동시에 여러 값을 예측하는 경우)의 경우 출력 차원마다 출력 뉴런이 하나씩 필요(예: 바운딩 박스, 좌표 2개, 너비 1개, 높이 1개. 총 출력 뉴런 4개 필요)\n",
    "- 출력 뉴런의 활성화 함수: 출력 값의 범위에 따라 사용하기도, 사용하지 않기도 함\n",
    "    - 활성화 함수: ReLU, softplus, sigmoid, tanh 등\n",
    "- 손실 함수: MSE,  MAE(Train set에 이상치가 많은 경우), Huber(MSE, MAE 조합)\n",
    "- 회귀 MLP의 구조 예시\n",
    "    - 입력 뉴런 수: 특성마다 하나씩\n",
    "    - 은닉층 수: 문제에 따라 다름. 1~5 사이\n",
    "    - 은닉층의 뉴런수: 문제에 따라 다름. 10~100 사이\n",
    "    - 은닉층의 활성화 함수: ReLU(또는 SELU)\n",
    "    - 출력 뉴런 수: 예측 차원마다 하나\n",
    "    - 출력층의 활성화 함수: 없거나 출력값의 범위에 따라 다르게 사용\n",
    "        - 출력이 양수일떄: ReLU, softplus\n",
    "        - 출력을 특정 범위로 제한시: sigmoid, tanh\n",
    "    - 손실함수: MSE, MAE/Huber(이상치가 있을시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 10.1.6 분류를 위한 다층 퍼셉트론\n",
    "- MLP의 출력값은 해당 클래스에 대한 예측 확률값\n",
    "    - 이진분류, 다중레이블 이진분류: 양성 클래스에 대한 예측 확률\n",
    "    - 다중분류: 해당 클래스에 대한 예측 확률(모든 클래스의 예측합은 1)\n",
    "\n",
    "- 분류 MLP의 구조 예시\n",
    "    - 입력층과 은닉층: 회귀와 동일\n",
    "    - 출력 뉴런 수: 1개(이진분류), 레이블마다 1개(다중레이블 이진분류), 클래스마다 1개(다중분류)\n",
    "    - 출력층의 활성화 함수: sigmoid(이진분류, 다중레이블 이진분류), softmax(다중분류)\n",
    "    - 손실함수: cross-entropy loss(=log loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 케라스로 다층 퍼셉트론 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.1 텐서플로 2 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 시퀀셜 API를 사용하여 이미지 분류기 만들기\n",
    "- fashion_mnist 데이터셋\\\n",
    "10개의 클래스로 이루어진 28X28 픽셀의 흑백 이미지 7만개\\\n",
    "MNIST와 형태가 같으나 손글씨 숫자가 아닌 패션 아이템 이미지\\\n",
    "클래스마다 샘플이 더 다양하며 MNIST보다 더 어려운 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스를 사용하여 데이터셋 적재하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n",
      "0 255\n"
     ]
    }
   ],
   "source": [
    "# X값 파악\n",
    "print(X_train_full.shape)\n",
    "print(X_train_full.dtype)\n",
    "print(X_train_full.min(), X_train_full.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y train, val, test 나누기 및 스케일 조정: [0, 1]의 float 형태로\n",
    "# X의 스케일 조정하는 이유: 경사하강법으로 신경망을 훈련하기 때문\n",
    "X_valid, X_train = X_train_full[:5000]/255, X_train_full[5000:]/255\n",
    "X_test = X_test/255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y의 클래스 이름 지정\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀셜 API를 사용하여 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 은닉층으로 이루어진 다중분류용 다층퍼셉트론(MLP)\n",
    "\n",
    "# 1. Sequential API: 순서대로 연결된 층을 일렬로 쌓아서 구성\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# 2. 1층(Flatten): 입력층. 입력 이미지를 1D 배열로 변환. 파라미터 없는 간단한 전처리 과정\n",
    "# 모델의 첫 층이기에 input_shape 지정. 배치 사이즈를 제외한 샘플의 크기만 써야함\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "\n",
    "# 3. 2층(Dense): 첫번째 Dense 은닉층. ReLU 사용. Dense층마다 가중치 행렬을 관리함\n",
    "# 각 층의 뉴런과 입력 사이의 모든 연결 가중치가 포함되며, 편향또한 벡터로 관리함\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "\n",
    "# 4. 3층(Dense): 두번째 Dense 은닉층\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# 5. 4층(Dense): 출력층. 뉴런 10개를 가진 Dense층을 추가\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential 모델을 만들 때, 위처럼 층을 하나씩 add하지 않고 리스트 형태로도 작성 가능\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 요약\n",
    "model.summary()\n",
    "\n",
    "# layer: 각 층의 이름\n",
    "# Output Shape: 출력크기. None: 배치 크기에 어떤 값도 가능하다는 의미\n",
    "# Param #: 파라미터 갯수. 훈련되는 파라미터와 훈련되지 않은 파라미터를 모두 포함\n",
    "# 1번째 은닉층의 파라미터 갯수: 784*300(연결 가중치) + 300(bias) = 235500\n",
    "# 2번째 은닉층의 파라미터 갯수: 300*100(연결 가중치) + 100(bias) = 30100\n",
    "# 출력층의 파라미터 갯수: 100*10(연결 가중치) + 10(bias) = 1010\n",
    "\n",
    "### 파라미터수가 많고, 훈련 데이터가 많지 않을시 과대적합의 위험이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.layers.core.flatten.Flatten object at 0x0000027B41825CD0>, <keras.layers.core.dense.Dense object at 0x0000027B41825A60>, <keras.layers.core.dense.Dense object at 0x0000027B418122E0>, <keras.layers.core.dense.Dense object at 0x0000027B41AE2250>]\n",
      "dense_14\n",
      "[[-4.36110049e-03  2.90213078e-02 -3.01976502e-02 ... -6.83602020e-02\n",
      "  -8.32527876e-03 -6.98106736e-02]\n",
      " [ 4.14008945e-02 -5.29699698e-02 -6.02805875e-02 ...  6.65770322e-02\n",
      "   3.39299589e-02  2.97228992e-02]\n",
      " [ 2.91691348e-02  3.25812995e-02 -3.19365449e-02 ...  1.40430927e-02\n",
      "  -1.41826011e-02  5.96647412e-02]\n",
      " ...\n",
      " [ 2.53277421e-02  1.43776685e-02  2.51225308e-02 ... -7.10561424e-02\n",
      "  -2.81205662e-02 -4.17584740e-02]\n",
      " [-3.16247307e-02  5.12631983e-02 -5.45949340e-02 ... -6.98876083e-02\n",
      "  -7.87556916e-03 -5.64955994e-02]\n",
      " [ 2.12868080e-02  3.73050570e-05  2.24471092e-02 ... -1.65431201e-02\n",
      "   1.92171708e-02  8.77465308e-03]]\n",
      "(784, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# 모델의 층 정보: 이름, 가중치(연결 가중치, 편향)\n",
    "print(model.layers)\n",
    "print(model.layers[1].name)\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(biases)\n",
    "print(biases.shape)\n",
    "\n",
    "# Dense층은 대칭성을 깨뜨리기 위해 연결 가중치를 무작위로 초기화 하고, 편향은 0으로 초기화 함\n",
    "# 다른 초기화 방법을 사용하고 싶다면 층을 만들때 매개변수로 kernel_initializer, bias_initializer 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성후 컴파일을 통해 손실함수, 옵티마이저 지정해야. 훈련 및 평가시 계산할 지표도 지정 가능\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics='acc')\n",
    "\n",
    "# 1. 손실함수: 레이블이 정수 1개, 클래스가 배타적이므로 sparse_categorical_crossentropy\n",
    "#              레이블이 샘플마다 클래스 별 타깃 확률을 가지고 있다면 categorical_crossentropy\n",
    "#              이진분류(다중 레이블 이진분류)는 binary_crossentropy\n",
    "# 2. 옵티마이저: sgd(Stochastic Gradient Descent: 확률적 경사하강법)\n",
    "# 3. 평가지표: 분류기이므로 accuracy로 지정. 'accuracy', ['accuracy'] 등 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7253 - acc: 0.7597 - val_loss: 0.5247 - val_acc: 0.8282\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4910 - acc: 0.8297 - val_loss: 0.4636 - val_acc: 0.8438\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4436 - acc: 0.8449 - val_loss: 0.4260 - val_acc: 0.8562\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4163 - acc: 0.8545 - val_loss: 0.4368 - val_acc: 0.8376\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3953 - acc: 0.8611 - val_loss: 0.4095 - val_acc: 0.8554\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3794 - acc: 0.8662 - val_loss: 0.3780 - val_acc: 0.8702\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3659 - acc: 0.8703 - val_loss: 0.3628 - val_acc: 0.8710\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3547 - acc: 0.8739 - val_loss: 0.3717 - val_acc: 0.8690\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3432 - acc: 0.8788 - val_loss: 0.3532 - val_acc: 0.8788\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3340 - acc: 0.8805 - val_loss: 0.3719 - val_acc: 0.8698\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3266 - acc: 0.8831 - val_loss: 0.3509 - val_acc: 0.8742\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3172 - acc: 0.8866 - val_loss: 0.3384 - val_acc: 0.8784\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3107 - acc: 0.8891 - val_loss: 0.3306 - val_acc: 0.8836\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3038 - acc: 0.8909 - val_loss: 0.3549 - val_acc: 0.8738\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2963 - acc: 0.8947 - val_loss: 0.3303 - val_acc: 0.8828\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2912 - acc: 0.8949 - val_loss: 0.3237 - val_acc: 0.8830\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2845 - acc: 0.8967 - val_loss: 0.3365 - val_acc: 0.8816\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2796 - acc: 0.9002 - val_loss: 0.3338 - val_acc: 0.8820\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2748 - acc: 0.9028 - val_loss: 0.3098 - val_acc: 0.8884\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2689 - acc: 0.9034 - val_loss: 0.3101 - val_acc: 0.8930\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2644 - acc: 0.9048 - val_loss: 0.3292 - val_acc: 0.8828\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2598 - acc: 0.9053 - val_loss: 0.3138 - val_acc: 0.8908\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2546 - acc: 0.9077 - val_loss: 0.3043 - val_acc: 0.8930\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2501 - acc: 0.9108 - val_loss: 0.3027 - val_acc: 0.8918\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2461 - acc: 0.9122 - val_loss: 0.3047 - val_acc: 0.8916\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2415 - acc: 0.9120 - val_loss: 0.3354 - val_acc: 0.8848\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2378 - acc: 0.9138 - val_loss: 0.3528 - val_acc: 0.8732\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2333 - acc: 0.9152 - val_loss: 0.3066 - val_acc: 0.8956\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2304 - acc: 0.9158 - val_loss: 0.3142 - val_acc: 0.8898\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2262 - acc: 0.9182 - val_loss: 0.3058 - val_acc: 0.8894\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "# model.fit()의 매개변수\n",
    "# validation_split: train의 해당 비율만큼을 validation으로 사용 가능\n",
    "# class_weight: 클래스에 따라 값의 편중이 되어 있는 경우 지정\n",
    "#               적게 등장하는 클래스는 높은 가중치, 많이 등장하는 클래스는 낮은 가중치를 부여함\n",
    "# sample_weight: 샘플별 가중치 부여\n",
    "\n",
    "# train set: 에포크가 진행되는 동안 평균 loss, acc 계산, val set: 에포크가 끝난 후 평균 loss, acc 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history 이용하여 학습곡선(learning curve) 그리기\n",
    "# history: history.history, history.params, history.epoch\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "print(len(history.history['loss']))\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# 훈련 epoch이 증가함에 따라 train acc, val acc 증가, train loss, val loss 는 감소\n",
    "# 검증곡선이 훈련곡선과 가까움 -> 과대적합 되지 않았음\n",
    "# 훈련곡선은 에포크가 진행되는 동안 계산, 검증곡선은 에포크가 끝난 후에 계산\n",
    "#   -> 훈련곡선은 에포크의 절반만큼 왼쪽으로 이동시켜 봐야 -> 훈련초기에 훈련곡선과 검증곡선이 거의 일치\n",
    "# epoch이 증가함에 따라 loss가 계속 감소한다면 모델이 완전히 수렴되지 않은것. 훈련 계속해야함\n",
    "\n",
    "# 모델 성능이 불만족스럽다면 하이퍼파라미터 튜닝 필요\n",
    "#   -> 학습률, 옵티마이저, 층 갯수, 층의 뉴런 갯수, 은닉층의 활성화 함수, 배치사이즈 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3386 - acc: 0.8814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33859431743621826, 0.8813999891281128]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set으로 모델 평가\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# 검증세트보다 테스트세트에서 성능이 조금 낮음(일반적)\n",
    "# 하이퍼파라미터 튜닝은 검증세트에서 진행해야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 사용해 예측을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "model.predict(X_new).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[model.predict(X_new).round(2)[0].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기\n",
    "- 시퀀셜 API를 사용한 회귀용 MLP는 분류용 MLP 구축, 훈련, 평가, 예측 방법과 거의 동일\n",
    "- 차이점: 출력층이 활성화 함수가 없는 하나의 뉴런을 가지며, 손실함수는 MSE 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3673913776874542"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.9377 - val_loss: 0.5245\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5114 - val_loss: 0.4364\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4115\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4296 - val_loss: 0.3923\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4166 - val_loss: 0.3863\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4068 - val_loss: 0.3767\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4025 - val_loss: 0.3736\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3678\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3713\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.3606\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.3538\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3785 - val_loss: 0.3549\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3753 - val_loss: 0.3515\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3734 - val_loss: 0.3712\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3459\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.3604\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.7769\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.3534\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3455\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3501\n",
      "162/162 [==============================] - 0s 904us/step - loss: 0.3827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2779453],\n",
       "       [2.8629484],\n",
       "       [3.617066 ]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋의 잡음이 많으므로, 과대적합을 막기 위해 뉴런수가 적은 은닉층 하나만 사용\n",
    "# 출력층: 활성화 함수가 없는 하나의 뉴런\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test= model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curve\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.4 함수형 API를 사용해 복잡한 모델 만들기\n",
    "- 함수형 API: 순차적인 Sequential API와는 달리 입력과 출력이 여러개거나 더 복잡한 네트워크 토폴로지를 갖는 신경망을 만들때 사용\n",
    "- 입력이 여러개인 경우\n",
    "    - 예) Wide & Deep 신경망\n",
    "    - 입력의 일부 또는 전체가 출력층에 바로 연결되는 형태\n",
    "    - 신경망이 복잡한 패턴(깊은층)과 간단한 패턴(짧은경로) 모두 학습 가능\n",
    "    - 데이터에 있는 간단한 패턴이 연속된 변환으로 왜곡될 가능성을 줄임(MLP의 한계 극복)\n",
    "- 출력이 여러개인 경우\n",
    "    - 그림에 있는 주요 물체를 분류하고 위치를 알아내는 경우. 회귀(물체 중심의 좌표, 너비, 높이)+분류 동시에\n",
    "    - 동일한 데이터에서 독립적인 여러 작업을 수행 예) 얼굴 사진으로 다중 작업 분류를 수행(표정, 안경여부)\n",
    "    - 규제기법: 신경망 구조 안에 보조 출력을 추가. 하위 네트워크가 나머지 네트워크에 의존하지 않고 그 자체로 유용한 것을 학습하는지 확인\n",
    "        - 규제: 과대적합 감소, 모델의 일반화 성능을 높이기 위해 훈련에 제약을 가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 30)           270         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 30)           930         ['dense_49[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 38)           0           ['input_5[0][0]',                \n",
      "                                                                  'dense_50[0][0]']               \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 1)            39          ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API 모델\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 30)           210         ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 30)           930         ['dense_52[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_53[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 입력: 일부 입력은 짧은 경로, 그 외 입력은 깊은 경로로 전달\n",
    "input_A = keras.layers.Input(shape=[5], name='wide_input') # 0~4 까지의 feature\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input') # 2~7 까지의 feature\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7970 - val_loss: 0.9045\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8512 - val_loss: 0.7377\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7442 - val_loss: 0.6654\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6863 - val_loss: 0.6192\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6459 - val_loss: 0.5847\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6144 - val_loss: 0.5574\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5892 - val_loss: 0.5343\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5678 - val_loss: 0.5181\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5039\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.4909\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4812\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5138 - val_loss: 0.4721\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5051 - val_loss: 0.4675\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.4601\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4918 - val_loss: 0.4543\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4859 - val_loss: 0.4492\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.4452\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.4423\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4723 - val_loss: 0.4380\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4679 - val_loss: 0.4351\n",
      "162/162 [==============================] - 0s 907us/step - loss: 0.4732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5221052],\n",
       "       [2.7898154],\n",
       "       [3.8417253]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 30)           210         ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 30)           930         ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_35[0][0]']               \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 1)            36          ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " aux_output (Dense)             (None, 1)            31          ['dense_35[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 출력: 규제를 위해 보조 출력을 추가하는 경우\n",
    "input_A = keras.layers.Input(shape=[5], name='wide_input') # 0~4 까지의 feature\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input') # 2~7 까지의 feature\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3412 - main_output_loss: 0.3300 - aux_output_loss: 0.4414 - val_loss: 0.3268 - val_main_output_loss: 0.3172 - val_aux_output_loss: 0.4129\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3386 - main_output_loss: 0.3274 - aux_output_loss: 0.4391 - val_loss: 0.3293 - val_main_output_loss: 0.3198 - val_aux_output_loss: 0.4150\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3364 - main_output_loss: 0.3255 - aux_output_loss: 0.4348 - val_loss: 0.3215 - val_main_output_loss: 0.3119 - val_aux_output_loss: 0.4081\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3350 - main_output_loss: 0.3243 - aux_output_loss: 0.4320 - val_loss: 0.3208 - val_main_output_loss: 0.3112 - val_aux_output_loss: 0.4079\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3369 - main_output_loss: 0.3264 - aux_output_loss: 0.4310 - val_loss: 0.3312 - val_main_output_loss: 0.3223 - val_aux_output_loss: 0.4114\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3338 - main_output_loss: 0.3235 - aux_output_loss: 0.4269 - val_loss: 0.3290 - val_main_output_loss: 0.3198 - val_aux_output_loss: 0.4118\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3328 - main_output_loss: 0.3227 - aux_output_loss: 0.4240 - val_loss: 0.3246 - val_main_output_loss: 0.3155 - val_aux_output_loss: 0.4066\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3310 - main_output_loss: 0.3209 - aux_output_loss: 0.4218 - val_loss: 0.3191 - val_main_output_loss: 0.3103 - val_aux_output_loss: 0.3981\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3311 - main_output_loss: 0.3214 - aux_output_loss: 0.4185 - val_loss: 0.3275 - val_main_output_loss: 0.3187 - val_aux_output_loss: 0.4066\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3318 - main_output_loss: 0.3221 - aux_output_loss: 0.4192 - val_loss: 0.3287 - val_main_output_loss: 0.3201 - val_aux_output_loss: 0.4059\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3286 - main_output_loss: 0.3190 - aux_output_loss: 0.4155 - val_loss: 0.3202 - val_main_output_loss: 0.3116 - val_aux_output_loss: 0.3974\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3288 - main_output_loss: 0.3192 - aux_output_loss: 0.4148 - val_loss: 0.3156 - val_main_output_loss: 0.3071 - val_aux_output_loss: 0.3927\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3290 - main_output_loss: 0.3198 - aux_output_loss: 0.4118 - val_loss: 0.3171 - val_main_output_loss: 0.3087 - val_aux_output_loss: 0.3924\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3292 - main_output_loss: 0.3199 - aux_output_loss: 0.4130 - val_loss: 0.3226 - val_main_output_loss: 0.3142 - val_aux_output_loss: 0.3980\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3307 - main_output_loss: 0.3218 - aux_output_loss: 0.4106 - val_loss: 0.3214 - val_main_output_loss: 0.3132 - val_aux_output_loss: 0.3947\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3264 - main_output_loss: 0.3175 - aux_output_loss: 0.4071 - val_loss: 0.3195 - val_main_output_loss: 0.3112 - val_aux_output_loss: 0.3940\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3252 - main_output_loss: 0.3163 - aux_output_loss: 0.4048 - val_loss: 0.3164 - val_main_output_loss: 0.3084 - val_aux_output_loss: 0.3890\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3261 - main_output_loss: 0.3175 - aux_output_loss: 0.4043 - val_loss: 0.3179 - val_main_output_loss: 0.3097 - val_aux_output_loss: 0.3909\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3252 - main_output_loss: 0.3166 - aux_output_loss: 0.4024 - val_loss: 0.3220 - val_main_output_loss: 0.3145 - val_aux_output_loss: 0.3895\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3246 - main_output_loss: 0.3161 - aux_output_loss: 0.4013 - val_loss: 0.3278 - val_main_output_loss: 0.3205 - val_aux_output_loss: 0.3936\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3400 - main_output_loss: 0.3312 - aux_output_loss: 0.4198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1.377639 ],\n",
       "        [2.7681932],\n",
       "        [3.800742 ]], dtype=float32),\n",
       " array([[1.2965128],\n",
       "        [2.7447767],\n",
       "        [3.6750295]], dtype=float32)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출력은 각각의 손실함수가 필요\n",
    "# 출력에 따른 가중치 부여도 가능함\n",
    "model.compile(loss=['mse','mse'], loss_weights=[0.9, 0.1], optimizer='sgd')\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "model.evaluate([X_test_A, X_test_B], [y_test, y_test])\n",
    "model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.5 서브클래싱 API로 동적 모델 만들기\n",
    "- 서브클래싱 API: 명령형(imperative) 프로그래밍\n",
    "    - 동적인 구조. 반복문 포함 가능, 다양한 크기를 다룰수 있고 조건문도 가질 수 있음\n",
    "- 선언적(declarative) 모델: Sequential, 함수형 API\n",
    "    - 사용할 층과 연결 방식을 미리 정의\n",
    "    - 모델을 저장, 복사, 공유가 쉬움\n",
    "    - 모델 구조 출력, 분석이 용이\n",
    "    - 프레임워크가 크기를 짐작하고 타입을 확인하여 모델에 데이터 주입 전 에러를 일찍 발견 가능\n",
    "    - 전체 모델이 층으로 구성되어 디버깅도 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 클래스를 상속 후 생성자 안에 필요한 층을 만듦\n",
    "# call() 메서드 안에 수행하려는 연산을 기술\n",
    "\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model1 = WideAndDeepModel()\n",
    "\n",
    "# Input 클래스의 객체를 만들 필요가 없음. 대신 call() 메서드의 input 매개변수를 사용함\n",
    "# 장점: 유연성이 높음. call()메서드 안에서 원하는 계산을 사용 가능(for, if, 텐서플로 저수준 연산 등등)\n",
    "# 단점: 모델 구조가 call()메서드 안에 숨겨져 있기 때문에 케라스가 쉽게 분석할 수 없음. 모델 저장, 복사 불가\n",
    "#       summary() 사용해도 층의 목록만 나열. 층 간의 연결 정보를 얻을 수 없음\n",
    "#       케라스가 타입과 크기를 미리 확인할 수 없어 실수 발생하기 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.6 모델 저장과 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 끝난 모델 저장(HDF5 포맷)\n",
    "# 모든 층의 하이퍼파라미터, 모델 구조, 각 층의 모든 모델 파라미터(연결 가중치와 편향), 옵티마이저 저장\n",
    "model.save('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " deep_input (InputLayer)        [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 30)           210         ['deep_input[0][0]']             \n",
      "                                                                                                  \n",
      " wide_input (InputLayer)        [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 30)           930         ['dense_47[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 35)           0           ['wide_input[0][0]',             \n",
      "                                                                  'dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            36          ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,176\n",
      "Trainable params: 1,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.7 콜백 사용하기\n",
    "- fit()의 callbacks: 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트 지정. 에포크의 시작이나 끝, 각 배체 처리 전후 호출도 가능\n",
    "    - ModelCheckpoint: 훈련하는 동안 일정한 간격으로 모델의 체크포인트(모델 파라미터를 저장하는 포맷)를 저장. 매 에포크의 끝에서 호출\n",
    "    - EarlyStopping: 일정 에포크(patience 매개변수로 지정)동안 검증세트 점수가 향상되지 않으면 훈련 종료. 선택적으로 최상의 모델 복원 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 978us/step - loss: 0.3668\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.3617\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 970us/step - loss: 0.3604\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.3595\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.3575\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 992us/step - loss: 0.3582\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.3556\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.3558\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5')\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3269\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3311\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3241\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3258\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3258\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3221\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3201\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3450 - val_loss: 0.3228\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3448 - val_loss: 0.3186\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 0.3206\n"
     ]
    }
   ],
   "source": [
    "# 훈련시 검증세트 사용하면 save_best_only=True 지정 가능\n",
    "# 최상의 검증세트 점수에서만 모델을 저장하므로, 훈련 세트에 과대적합되지 않음\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model('my_keras_model.h5') # 최상의 모델로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3411 - val_loss: 0.3251\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3170\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3258\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.3188\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3222\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3160\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3134\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3150\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3155\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3166\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3137\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3137\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3344 - val_loss: 0.3143\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3172\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3330 - val_loss: 0.3125\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.3124\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3310 - val_loss: 0.3097\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3097\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3094\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3108\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3084\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3264 - val_loss: 0.3067\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3105\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3073\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3072\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3083\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3081\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3291 - val_loss: 0.3104\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.3045\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3014\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3035\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3054\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3211 - val_loss: 0.3097\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3229 - val_loss: 0.3027\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3051\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3229 - val_loss: 0.3019\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3029\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.3162\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3029\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3050\n"
     ]
    }
   ],
   "source": [
    "# 조기종료 구현: EarlyStopping 콜백 사용\n",
    "# 일정 에포크(patience 매개변수로 지정)동안 검증세트 점수가 향상되지 않으면 훈련 종료. 선택적으로 최상의 모델 복원 가능\n",
    "# 체크포인트 저장 콜백, EarlyStopping 콜백을 동시에 사용 가능\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.8 텐서보드를 사용해 시각화하기\n",
    "- 텐서보드: 인터렉티브 시각화 도구. 텐서플로 설치시 자동으로 설치됨\n",
    "    - 기능: 훈련하는 동안 학습곡선 그리기, 여러 실행간의 학습곡선 비교, 계산그래프 시각화, 훈련 통계분석\\\n",
    "            모델이 생성한 이미지 확인, 3D에 투영된 복잡한 다차원 데이터 시각화, 클러스터링 등등\n",
    "- 루트로그 디렉터리(텐서보드 서버), 서브 디렉터리(이벤트 기록) 정의 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7d17f96c5f5cbc1a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7d17f96c5f5cbc1a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3225 - val_loss: 0.3063\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3231 - val_loss: 0.3040\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3259 - val_loss: 0.3056\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3236 - val_loss: 0.3024\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3032\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3083\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3216 - val_loss: 0.3041\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3035\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3056\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3023\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3021\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3200 - val_loss: 0.3000\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3039\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3027\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3021\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3175 - val_loss: 0.2996\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3177 - val_loss: 0.3034\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3097\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3226 - val_loss: 0.3015\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.2999\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - val_loss: 0.3031\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - ETA: 0s - loss: 0.315 - 0s 1ms/step - loss: 0.3174 - val_loss: 0.2999\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.2998\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.3012\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3095\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.2989\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.2978\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3142 - val_loss: 0.2975\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3017\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3054\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 신경망 하이퍼파라미터 튜닝하기\n",
    "- 최적의 하이퍼파라미터 찾기: 많은 파라미터 조합을 시도해보고, 어떤 것이 검증 세트(혹은 k-fold)에서 가장 좋은 점수를 내는지 확인\n",
    "- GridSearchCV, RandomizedSearchCV를 사용해 하이퍼 파라미터 공간 탐색 <- 케라스 모델을 사이킷런 추정기처럼 보이도록 바꿔야\n",
    "- 하이퍼파라미터 공간 탐색: 탐색 지역이 좋다고 판명될 때 더 탐색을 수행하며 탐색 공간을 줄여나가는 방식\n",
    "- 하이퍼파라미터 최적화에 사용하는 파이썬 라이브러리\n",
    "    - Hyperopt: 모든 종류의 복잡한 탐색 공간에 대해 최적화 수행\n",
    "    - Hyperas, kopt, Talos: 케라스 모델을 위한 하이퍼파라미터 최적화 라이브러리\n",
    "    - Keras Tuner: 사용하기 쉬운 케라스 하이퍼파라미터 최적화 라이브러리. 구글이 만들었고 시각화, 분석 포함한 클라우드 서비스 제공 예정\n",
    "    - Scikit-Optimize(skopt): 범용 최적화 라이브러리. BayesSearchCV 클래스는 GridSearchCV와 비슷한 인터페이스를 사용하여 베이즈 최적화 수행\n",
    "    - Spearmint: 베이즈 최적화 라이브러리\n",
    "    - Hyperband: Hyperband 논문을 기반으로 구축된 빠른 하이퍼파라미터 튜닝 라이브러리\n",
    "    - Sklearn-Deap: GridSearchCV와 비슷한 인터페이스를 가진 진화알고리즘 기반의 하이퍼파라미터 최적화 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential MLP 회귀 모델 만드는 함수 작성\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-196-6dfaa292f3b6>:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "# 케라스 모델을 감싸는 래퍼 객체 생성. 일반적인 사이킷런 회귀 모델처럼 객체 사용 가능\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2161 - val_loss: 0.5999\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5182\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5384 - val_loss: 0.4783\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5080 - val_loss: 0.4588\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4896 - val_loss: 0.4396\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4668 - val_loss: 0.4358\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4602 - val_loss: 0.4227\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4487 - val_loss: 0.4181\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4490 - val_loss: 0.4150\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4394 - val_loss: 0.4053\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4309 - val_loss: 0.4035\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4023\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.3975\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.3923\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3910\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.3904\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.3862\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.3833\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3821\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.3834\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.3794\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.3769\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3720\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.3713\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.3710\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.3690\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.3670\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3897 - val_loss: 0.3677\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3884 - val_loss: 0.3633\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.3610\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3931 - val_loss: 0.3602\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.3614\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.3620\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.3570\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.3620\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.3570\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3850 - val_loss: 0.3560\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3532\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3548\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.3520\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3756 - val_loss: 0.3510\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3504\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3545\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3762 - val_loss: 0.3487\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3717 - val_loss: 0.3471\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3764 - val_loss: 0.3488\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3869 - val_loss: 0.3477\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3723 - val_loss: 0.3464\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3709 - val_loss: 0.3469\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3704 - val_loss: 0.3454\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3438\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3452\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3424\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3630 - val_loss: 0.3415\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3678 - val_loss: 0.3437\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.3416\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3635 - val_loss: 0.3407\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3604 - val_loss: 0.3396\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3404\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3382\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3632 - val_loss: 0.3427\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - val_loss: 0.3427\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3599 - val_loss: 0.3454\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3693 - val_loss: 0.3425\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3696 - val_loss: 0.3456\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3461\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3377\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3627 - val_loss: 0.3383\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3603 - val_loss: 0.3366\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.3348\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3560 - val_loss: 0.3343\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3523 - val_loss: 0.3370\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3555 - val_loss: 0.3348\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3527 - val_loss: 0.3321\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3368\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3522 - val_loss: 0.3320\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3514 - val_loss: 0.3357\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.3354\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3325\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3302\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3305\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3285\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3311\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3567 - val_loss: 0.3291\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3297\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3274\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3315\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3271\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3272\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3310\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3244\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.3278\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3292\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3239\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3238\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3250\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3235\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.3225\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3221\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3412 - val_loss: 0.3245\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3744\n"
     ]
    }
   ],
   "source": [
    "# fit()메서드에 지정한 모든 매개변수는 케라스 모델로 전달됨\n",
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 6.5014 - val_loss: 4.9249\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2437 - val_loss: 3.4361\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.0707 - val_loss: 2.5817\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.3710 - val_loss: 2.0424\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9213 - val_loss: 1.6866\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6186 - val_loss: 1.4427\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4080 - val_loss: 1.2735\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2571 - val_loss: 1.1508\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1458 - val_loss: 1.0606\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0613 - val_loss: 0.9918\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9958 - val_loss: 0.9387\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9438 - val_loss: 0.8963\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9020 - val_loss: 0.8617\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8680 - val_loss: 0.8330\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8399 - val_loss: 0.8088\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8163 - val_loss: 0.7879\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7963 - val_loss: 0.7700\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7791 - val_loss: 0.7540\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7640 - val_loss: 0.7397\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7506 - val_loss: 0.7269\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7386 - val_loss: 0.7152\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7278 - val_loss: 0.7045\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7179 - val_loss: 0.6947\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7089 - val_loss: 0.6856\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7006 - val_loss: 0.6771\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6692\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6855 - val_loss: 0.6618\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6786 - val_loss: 0.6547\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6721 - val_loss: 0.6481\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6660 - val_loss: 0.6418\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6602 - val_loss: 0.6359\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6546 - val_loss: 0.6302\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6493 - val_loss: 0.6248\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6442 - val_loss: 0.6197\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6393 - val_loss: 0.6148\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6346 - val_loss: 0.6101\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6301 - val_loss: 0.6057\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6257 - val_loss: 0.6014\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.5972\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6176 - val_loss: 0.5932\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6138 - val_loss: 0.5894\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6100 - val_loss: 0.5856\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6062 - val_loss: 0.5820\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.5785\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5992 - val_loss: 0.5751\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5958 - val_loss: 0.5719\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5926 - val_loss: 0.5687\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5657\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5627\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.5599\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.5572\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5778 - val_loss: 0.5546\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5752 - val_loss: 0.5521\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5726 - val_loss: 0.5496\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5701 - val_loss: 0.5473\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5677 - val_loss: 0.5451\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5654 - val_loss: 0.5429\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5631 - val_loss: 0.5408\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5609 - val_loss: 0.5387\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5588 - val_loss: 0.5368\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5567 - val_loss: 0.5349\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5547 - val_loss: 0.5331\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5527 - val_loss: 0.5313\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5509 - val_loss: 0.5297\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5280\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5473 - val_loss: 0.5265\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5456 - val_loss: 0.5250\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5441 - val_loss: 0.5236\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5425 - val_loss: 0.5223\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5210\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.5197\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5383 - val_loss: 0.5184\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5369 - val_loss: 0.5172\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5161\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5150\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5330 - val_loss: 0.5139\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5319 - val_loss: 0.5129\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5307 - val_loss: 0.5119\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.5109\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5285 - val_loss: 0.5100\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5091\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.5082\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.5074\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 0.5065\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 0.5057\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 0.5049\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5041\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 0.5033\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.5026\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5195 - val_loss: 0.5019\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.5011\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.5004\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.4997\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.4991\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5158 - val_loss: 0.4984\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5150 - val_loss: 0.4977\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.4971\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.4965\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5129 - val_loss: 0.4958\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5122 - val_loss: 0.4952\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5313\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.8590 - val_loss: 3.0535\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.7829 - val_loss: 2.2573\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1040 - val_loss: 1.7370\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6573 - val_loss: 1.3924\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3611 - val_loss: 1.1657\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1670 - val_loss: 1.0202\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0422 - val_loss: 0.9291\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.8720\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9111 - val_loss: 0.8349\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8761 - val_loss: 0.8091\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8508 - val_loss: 0.7903\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8313 - val_loss: 0.7752\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8156 - val_loss: 0.7626\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8023 - val_loss: 0.7517\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7907 - val_loss: 0.7419\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7803 - val_loss: 0.7328\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7708 - val_loss: 0.7245\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7620 - val_loss: 0.7167\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7538 - val_loss: 0.7093\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7462 - val_loss: 0.7023\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7390 - val_loss: 0.6957\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7320 - val_loss: 0.6893\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7254 - val_loss: 0.6831\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7192 - val_loss: 0.6772\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7131 - val_loss: 0.6715\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7073 - val_loss: 0.6660\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7017 - val_loss: 0.6606\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6961 - val_loss: 0.6555\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6908 - val_loss: 0.6504\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6855 - val_loss: 0.6456\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6805 - val_loss: 0.6408\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6756 - val_loss: 0.6363\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6708 - val_loss: 0.6318\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6662 - val_loss: 0.6274\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6618 - val_loss: 0.6232\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6575 - val_loss: 0.6190\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6532 - val_loss: 0.6150\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6491 - val_loss: 0.6111\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6451 - val_loss: 0.6073\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6412 - val_loss: 0.6036\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6374 - val_loss: 0.6000\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6337 - val_loss: 0.5965\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6301 - val_loss: 0.5931\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6266 - val_loss: 0.5897\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6232 - val_loss: 0.5863\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6198 - val_loss: 0.5831\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6164 - val_loss: 0.5799\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6132 - val_loss: 0.5767\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6100 - val_loss: 0.5737\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.5707\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6038 - val_loss: 0.5677\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6008 - val_loss: 0.5648\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5620\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.5592\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5922 - val_loss: 0.5564\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5538\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.5512\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5486\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5461\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5436\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5763 - val_loss: 0.5412\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5739 - val_loss: 0.5389\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5715 - val_loss: 0.5366\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5691 - val_loss: 0.5343\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5668 - val_loss: 0.5321\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5645 - val_loss: 0.5299\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5623 - val_loss: 0.5278\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5601 - val_loss: 0.5257\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5579 - val_loss: 0.5236\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5558 - val_loss: 0.5216\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5538 - val_loss: 0.5196\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5177\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5497 - val_loss: 0.5158\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.5139\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5121\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 0.5102\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5419 - val_loss: 0.5085\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5067\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5382 - val_loss: 0.5050\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5364 - val_loss: 0.5033\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5346 - val_loss: 0.5016\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5000\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4984\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5295 - val_loss: 0.4968\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.4952\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 0.4937\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.4922\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.4907\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5216 - val_loss: 0.4892\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.4878\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4864\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.4850\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5156 - val_loss: 0.4836\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5142 - val_loss: 0.4823\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.4810\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5115 - val_loss: 0.4797\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.4784\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.4772\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5075 - val_loss: 0.4759\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5062 - val_loss: 0.4746\n",
      "121/121 [==============================] - 0s 936us/step - loss: 0.5011\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.5087 - val_loss: 4.2591\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8808 - val_loss: 3.2282\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.0719 - val_loss: 2.6572\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5636 - val_loss: 2.2681\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1879 - val_loss: 1.9586\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.8817 - val_loss: 1.6978\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6222 - val_loss: 1.4717\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4011 - val_loss: 1.2757\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2135 - val_loss: 1.1120\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0599 - val_loss: 0.9791\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.8793\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8619 - val_loss: 0.8049\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8027 - val_loss: 0.7509\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7603 - val_loss: 0.7120\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7296 - val_loss: 0.6832\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7068 - val_loss: 0.6620\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6897 - val_loss: 0.6462\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6763 - val_loss: 0.6336\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6654 - val_loss: 0.6233\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6563 - val_loss: 0.6150\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6486 - val_loss: 0.6078\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6420 - val_loss: 0.6018\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6361 - val_loss: 0.5964\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6308 - val_loss: 0.5916\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6260 - val_loss: 0.5872\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.5834\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6177 - val_loss: 0.5798\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6139 - val_loss: 0.5763\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6104 - val_loss: 0.5730\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6072 - val_loss: 0.5701\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 0.5674\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6013 - val_loss: 0.5647\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.5621\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5960 - val_loss: 0.5597\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5935 - val_loss: 0.5573\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.5551\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5888 - val_loss: 0.5529\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5507\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5845 - val_loss: 0.5489\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5470\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.5450\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5431\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5767 - val_loss: 0.5413\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5748 - val_loss: 0.5395\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5376\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5712 - val_loss: 0.5359\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5695 - val_loss: 0.5342\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5678 - val_loss: 0.5325\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5662 - val_loss: 0.5309\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5645 - val_loss: 0.5294\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5629 - val_loss: 0.5278\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5614 - val_loss: 0.5263\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5599 - val_loss: 0.5249\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5583 - val_loss: 0.5233\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5569 - val_loss: 0.5219\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5554 - val_loss: 0.5205\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5540 - val_loss: 0.5191\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5525 - val_loss: 0.5177\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.5162\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5149\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5484 - val_loss: 0.5136\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5470 - val_loss: 0.5123\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5111\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5444 - val_loss: 0.5099\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5431 - val_loss: 0.5087\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5418 - val_loss: 0.5075\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5406 - val_loss: 0.5063\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5393 - val_loss: 0.5050\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5380 - val_loss: 0.5038\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5027\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5356 - val_loss: 0.5015\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5344 - val_loss: 0.5004\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5332 - val_loss: 0.4992\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.4982\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5309 - val_loss: 0.4971\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5298 - val_loss: 0.4961\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5286 - val_loss: 0.4950\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.4940\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5264 - val_loss: 0.4930\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.4919\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 0.4910\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.4900\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.4889\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5210 - val_loss: 0.4879\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5199 - val_loss: 0.4868\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.4859\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5179 - val_loss: 0.4850\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 0.4841\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5158 - val_loss: 0.4832\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5149 - val_loss: 0.4823\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5139 - val_loss: 0.4813\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5129 - val_loss: 0.4805\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5119 - val_loss: 0.4796\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5110 - val_loss: 0.4787\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5101 - val_loss: 0.4779\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5092 - val_loss: 0.4770\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5083 - val_loss: 0.4761\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5074 - val_loss: 0.4753\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5065 - val_loss: 0.4744\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5056 - val_loss: 0.4736\n",
      "121/121 [==============================] - 0s 948us/step - loss: 0.5246\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.0929 - val_loss: 2.0123\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6652 - val_loss: 1.2092\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1473 - val_loss: 0.9099\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9298 - val_loss: 0.7827\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8261 - val_loss: 0.7220\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7688 - val_loss: 0.6871\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7329 - val_loss: 0.6646\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7089 - val_loss: 0.6484\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6911 - val_loss: 0.6358\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6770 - val_loss: 0.6255\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6652 - val_loss: 0.6163\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6552 - val_loss: 0.6081\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6460 - val_loss: 0.6004\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6378 - val_loss: 0.5935\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6302 - val_loss: 0.5869\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6230 - val_loss: 0.5806\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6163 - val_loss: 0.5748\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6098 - val_loss: 0.5692\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6037 - val_loss: 0.5639\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5589\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5924 - val_loss: 0.5540\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5493\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5818 - val_loss: 0.5447\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.5405\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5363\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5676 - val_loss: 0.5322\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5632 - val_loss: 0.5282\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5590 - val_loss: 0.5245\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5550 - val_loss: 0.5208\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.5175\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5144\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 0.5112\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5405 - val_loss: 0.5081\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5371 - val_loss: 0.5051\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5024\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.4997\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5280 - val_loss: 0.4971\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5252 - val_loss: 0.4947\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.4923\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5198 - val_loss: 0.4902\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.4878\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.4858\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5125 - val_loss: 0.4836\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5102 - val_loss: 0.4815\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5079 - val_loss: 0.4796\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.4779\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5037 - val_loss: 0.4759\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5016 - val_loss: 0.4740\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4997 - val_loss: 0.4725\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.4710\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4961 - val_loss: 0.4694\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.4676\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4924 - val_loss: 0.4661\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4907 - val_loss: 0.4645\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4890 - val_loss: 0.4630\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.4619\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.4606\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4844 - val_loss: 0.4593\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4829 - val_loss: 0.4580\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4815 - val_loss: 0.4569\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.4556\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.4544\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4775 - val_loss: 0.4535\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.4525\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4748 - val_loss: 0.4512\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4501\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4492\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4712 - val_loss: 0.4482\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.4472\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4690 - val_loss: 0.4463\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4452\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4669 - val_loss: 0.4446\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4658 - val_loss: 0.4435\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4649 - val_loss: 0.4429\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4639 - val_loss: 0.4419\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4409\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4620 - val_loss: 0.4402\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.4395\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4386\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4379\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.4371\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4363\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4566 - val_loss: 0.4357\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4558 - val_loss: 0.4350\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4344\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4541 - val_loss: 0.4337\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4533 - val_loss: 0.4331\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4525 - val_loss: 0.4323\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4316\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4509 - val_loss: 0.4308\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4502 - val_loss: 0.4302\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.4294\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4289\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4283\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4473 - val_loss: 0.4276\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4271\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4266\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4260\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4445 - val_loss: 0.4255\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4438 - val_loss: 0.4247\n",
      "121/121 [==============================] - 0s 920us/step - loss: 0.4700\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.7299 - val_loss: 2.2545\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7012 - val_loss: 1.1710\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0675 - val_loss: 0.8452\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8545 - val_loss: 0.7395\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7735 - val_loss: 0.6983\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7371 - val_loss: 0.6772\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7167 - val_loss: 0.6629\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7024 - val_loss: 0.6511\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6912 - val_loss: 0.6410\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6813 - val_loss: 0.6317\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6725 - val_loss: 0.6230\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6641 - val_loss: 0.6151\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6563 - val_loss: 0.6074\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6490 - val_loss: 0.6001\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6418 - val_loss: 0.5931\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6350 - val_loss: 0.5864\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6285 - val_loss: 0.5801\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6223 - val_loss: 0.5741\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6164 - val_loss: 0.5684\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6104 - val_loss: 0.5629\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6051 - val_loss: 0.5576\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5996 - val_loss: 0.5525\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5947 - val_loss: 0.5477\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5897 - val_loss: 0.5430\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 0.5386\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5344\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5303\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.5264\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 0.5226\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5640 - val_loss: 0.5191\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5154\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 0.5120\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5532 - val_loss: 0.5089\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5057\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5465 - val_loss: 0.5027\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.4997\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5404 - val_loss: 0.4970\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5374 - val_loss: 0.4942\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5346 - val_loss: 0.4916\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.4892\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.4870\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.4847\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5242 - val_loss: 0.4823\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.4801\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.4780\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5175 - val_loss: 0.4763\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.4742\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5134 - val_loss: 0.4726\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5114 - val_loss: 0.4707\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5096 - val_loss: 0.4690\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5076 - val_loss: 0.4672\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.4657\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5040 - val_loss: 0.4639\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5023 - val_loss: 0.4627\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5007 - val_loss: 0.4609\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4992 - val_loss: 0.4595\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.4583\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4961 - val_loss: 0.4568\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4947 - val_loss: 0.4557\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4934 - val_loss: 0.4544\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4920 - val_loss: 0.4530\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4906 - val_loss: 0.4517\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4894 - val_loss: 0.4505\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.4495\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4868 - val_loss: 0.4486\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.4473\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.4461\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4834 - val_loss: 0.4453\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.4441\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4812 - val_loss: 0.4432\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.4424\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4791 - val_loss: 0.4415\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4780 - val_loss: 0.4405\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4770 - val_loss: 0.4396\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4760 - val_loss: 0.4386\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4751 - val_loss: 0.4379\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4741 - val_loss: 0.4369\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4733 - val_loss: 0.4361\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4724 - val_loss: 0.4353\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4714 - val_loss: 0.4345\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4706 - val_loss: 0.4339\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4697 - val_loss: 0.4330\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4687 - val_loss: 0.4322\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4314\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4672 - val_loss: 0.4309\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4663 - val_loss: 0.4304\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4298\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4649 - val_loss: 0.4289\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4641 - val_loss: 0.4282\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4630 - val_loss: 0.4276\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4626 - val_loss: 0.4269\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4616 - val_loss: 0.4261\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4256\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4250\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4596 - val_loss: 0.4244\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4238\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4581 - val_loss: 0.4232\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4574 - val_loss: 0.4225\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4567 - val_loss: 0.4219\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4561 - val_loss: 0.4214\n",
      "121/121 [==============================] - 0s 902us/step - loss: 0.4405\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.2385 - val_loss: 2.7624\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.1646 - val_loss: 1.6044\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4187 - val_loss: 1.1508\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0877 - val_loss: 0.9354\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9150 - val_loss: 0.8204\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8175 - val_loss: 0.7537\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7596 - val_loss: 0.7120\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7230 - val_loss: 0.6842\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6981 - val_loss: 0.6644\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6803 - val_loss: 0.6491\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6666 - val_loss: 0.6367\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6553 - val_loss: 0.6261\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6455 - val_loss: 0.6168\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6369 - val_loss: 0.6085\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6290 - val_loss: 0.6009\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.5939\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6148 - val_loss: 0.5874\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.5814\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6024 - val_loss: 0.5756\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5968 - val_loss: 0.5702\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5652\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5603\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5556\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.5515\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5724 - val_loss: 0.5473\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5683 - val_loss: 0.5434\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5642 - val_loss: 0.5397\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5605 - val_loss: 0.5360\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 0.5327\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5533 - val_loss: 0.5294\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.5262\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 0.5233\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5437 - val_loss: 0.5204\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5177\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5152\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5351 - val_loss: 0.5125\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5325 - val_loss: 0.5100\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5077\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5276 - val_loss: 0.5055\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5252 - val_loss: 0.5034\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 0.5013\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.4993\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.4974\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.4955\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5147 - val_loss: 0.4937\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.4920\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5110 - val_loss: 0.4902\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5092 - val_loss: 0.4886\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5075 - val_loss: 0.4869\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.4856\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5043 - val_loss: 0.4840\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.4825\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5013 - val_loss: 0.4811\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4998 - val_loss: 0.4798\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4984 - val_loss: 0.4784\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4971 - val_loss: 0.4772\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4958 - val_loss: 0.4760\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4945 - val_loss: 0.4748\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4932 - val_loss: 0.4736\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4920 - val_loss: 0.4725\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4908 - val_loss: 0.4713\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4896 - val_loss: 0.4703\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4885 - val_loss: 0.4692\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4874 - val_loss: 0.4682\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4863 - val_loss: 0.4672\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4853 - val_loss: 0.4661\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4843 - val_loss: 0.4651\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4833 - val_loss: 0.4641\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.4633\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.4626\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4804 - val_loss: 0.4616\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4795 - val_loss: 0.4607\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4786 - val_loss: 0.4599\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4777 - val_loss: 0.4589\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.4581\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.4574\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4752 - val_loss: 0.4565\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4744 - val_loss: 0.4557\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4550\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4729 - val_loss: 0.4542\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 0.4535\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4714 - val_loss: 0.4529\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4706 - val_loss: 0.4522\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.4515\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4692 - val_loss: 0.4509\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4685 - val_loss: 0.4501\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.4496\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4488\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4664 - val_loss: 0.4482\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4658 - val_loss: 0.4476\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.4469\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4644 - val_loss: 0.4462\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4457\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4631 - val_loss: 0.4451\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4445\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4619 - val_loss: 0.4439\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4612 - val_loss: 0.4431\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4606 - val_loss: 0.4426\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4420\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4413\n",
      "121/121 [==============================] - 0s 943us/step - loss: 0.4792\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1387 - val_loss: 1.3186\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3357 - val_loss: 1.3094\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3318 - val_loss: 1.3093\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3317 - val_loss: 1.3094\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3316 - val_loss: 1.3100\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3316 - val_loss: 1.3098\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3313 - val_loss: 1.3118\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3317 - val_loss: 1.3102\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3311 - val_loss: 1.3120\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3314 - val_loss: 1.3090\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3316 - val_loss: 1.3102\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3310 - val_loss: 1.3088\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3313 - val_loss: 1.3084\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3310 - val_loss: 1.3104\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3309 - val_loss: 1.3084\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3307 - val_loss: 1.3077\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3306 - val_loss: 1.3081\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3299 - val_loss: 1.3080\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3288 - val_loss: 1.3052\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3255 - val_loss: 1.2992\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3168 - val_loss: 1.2835\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2791 - val_loss: 1.2065\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1548 - val_loss: 1.0132\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9462 - val_loss: 0.8300\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8034 - val_loss: 0.7224\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7046 - val_loss: 0.6396\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 0.5881\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5395\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5441 - val_loss: 0.5124\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5193 - val_loss: 0.5023\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5317\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5072\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4910\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.4779\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4878 - val_loss: 0.4728\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4791 - val_loss: 0.4592\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4739 - val_loss: 0.4586\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.4497\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4768\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4887 - val_loss: 0.4639\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4767 - val_loss: 0.4532\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4664 - val_loss: 0.4427\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4396\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4544 - val_loss: 0.4349\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4563 - val_loss: 0.4402\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4550 - val_loss: 0.4340\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4489 - val_loss: 0.4255\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4667 - val_loss: 0.4450\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4572 - val_loss: 0.4355\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4499 - val_loss: 0.4305\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4254\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4229\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4767 - val_loss: 0.4497\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4407\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4313\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4263\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4181\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4141\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4097\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4249 - val_loss: 0.4081\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4221 - val_loss: 0.4018\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4313 - val_loss: 0.4169\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4106\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4036\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4002\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4029\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3944\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.3918\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.3897\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.3913\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.3994\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4089 - val_loss: 0.3906\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.4072\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.3913\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.3879\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.3896\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.3906\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.3862\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.3868\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3845\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3842\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.3884\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3869\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3820\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3982 - val_loss: 0.3818\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.3806\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.3844\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.3826\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3982 - val_loss: 0.3805\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.3824\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 0.3851\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.3825\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3964 - val_loss: 0.3803\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3837\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4154\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3814\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3956\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3962 - val_loss: 0.3807\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3982 - val_loss: 0.3893\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4225 - val_loss: 0.3910\n",
      "121/121 [==============================] - 0s 983us/step - loss: 0.4388\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1120 - val_loss: 1.3157\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3326 - val_loss: 1.3032\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3247 - val_loss: 1.2936\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3073 - val_loss: 1.2687\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2820 - val_loss: 1.2389\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2384 - val_loss: 1.1710\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1431 - val_loss: 1.0173\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9781 - val_loss: 0.8417\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8389 - val_loss: 0.7379\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7367 - val_loss: 0.6494\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6963 - val_loss: 0.6274\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6349 - val_loss: 0.5743\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.5459\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5239\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5070\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5265 - val_loss: 0.4936\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5848 - val_loss: 0.5307\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5528 - val_loss: 0.5074\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.4934\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5174 - val_loss: 0.4789\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5031 - val_loss: 0.4715\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.4601\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4807 - val_loss: 0.4512\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 0.4454\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4412\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4341\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4523 - val_loss: 0.4258\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4335\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.4877\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.4542\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4751 - val_loss: 0.4415\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4602 - val_loss: 0.4258\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4503 - val_loss: 0.4163\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4407 - val_loss: 0.4125\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4335 - val_loss: 0.4024\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4270 - val_loss: 0.3965\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.3999\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.3931\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4130 - val_loss: 0.3860\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.3812\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.3805\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.3801\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.3734\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.3785\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3728\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3724\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.3745\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3984 - val_loss: 0.3729\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.3738\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.3768\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.3751\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.3709\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.3713\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3706\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.3767\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3954 - val_loss: 0.3709\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 0.3701\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3936 - val_loss: 0.3706\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3947 - val_loss: 0.3675\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.3685\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3706\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.3715\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3907 - val_loss: 0.3740\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.3713\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.3692\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3864 - val_loss: 0.3670\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3873 - val_loss: 0.3672\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3859 - val_loss: 0.3644\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 0.3668\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 0.3624\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3695\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.3669\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3829 - val_loss: 0.3683\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.3686\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.3675\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.3645\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3652\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3613\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3772\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3665\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.3681\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3816 - val_loss: 0.3640\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3599\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3807 - val_loss: 0.3623\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3589\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3809 - val_loss: 0.3659\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3612\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.3627\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.3712\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3802 - val_loss: 0.3606\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3691\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.3588\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3623\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3669\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3807 - val_loss: 0.3654\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.3621\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.3599\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.3694\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3604\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3572\n",
      "121/121 [==============================] - 0s 978us/step - loss: 0.3640\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1046 - val_loss: 1.2846\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0947 - val_loss: 0.9444\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8495 - val_loss: 0.7521\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7201 - val_loss: 0.6558\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6450 - val_loss: 0.5958\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5555\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5568 - val_loss: 0.5288\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5328 - val_loss: 0.5069\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.4987\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.4857\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4993 - val_loss: 0.4805\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4968 - val_loss: 0.4757\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4872 - val_loss: 0.4653\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4846 - val_loss: 0.4653\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4789 - val_loss: 0.4670\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4746 - val_loss: 0.4504\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.4496\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4645 - val_loss: 0.4404\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4573 - val_loss: 0.4385\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4540 - val_loss: 0.4258\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4210\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4170\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4118\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4304 - val_loss: 0.4070\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4049\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4004\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4047\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.3866\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.3972\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.3897\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4074 - val_loss: 0.3799\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.3812\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.3743\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.3728\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3986 - val_loss: 0.3739\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3706\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3783\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.3686\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3777\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3681\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3941 - val_loss: 0.3673\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.3755\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.3784\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3897 - val_loss: 0.3705\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.3712\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.3781\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.3690\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3680\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.3700\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3885 - val_loss: 0.3720\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3905 - val_loss: 0.3725\n",
      "121/121 [==============================] - 0s 927us/step - loss: 0.3964\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6558 - val_loss: 1.2575\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0061 - val_loss: 0.8007\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7701 - val_loss: 0.7015\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7017 - val_loss: 0.6564\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6679 - val_loss: 0.6277\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6431 - val_loss: 0.6040\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6222 - val_loss: 0.5845\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6036 - val_loss: 0.5675\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5519\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5715 - val_loss: 0.5386\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5585 - val_loss: 0.5266\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5462 - val_loss: 0.5158\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5067\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5259 - val_loss: 0.4976\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.4899\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.4830\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5012 - val_loss: 0.4765\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4945 - val_loss: 0.4708\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4883 - val_loss: 0.4657\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4827 - val_loss: 0.4607\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4566\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4730 - val_loss: 0.4526\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4682 - val_loss: 0.4484\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4641 - val_loss: 0.4448\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4602 - val_loss: 0.4412\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4378\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4350\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4319\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4294\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4437 - val_loss: 0.4270\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4406 - val_loss: 0.4242\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4220\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4352 - val_loss: 0.4198\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.4173\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4304 - val_loss: 0.4151\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4280 - val_loss: 0.4130\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4255 - val_loss: 0.4110\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4235 - val_loss: 0.4090\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4215 - val_loss: 0.4074\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4194 - val_loss: 0.4055\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4174 - val_loss: 0.4041\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4156 - val_loss: 0.4022\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4135 - val_loss: 0.4011\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4119 - val_loss: 0.3995\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4104 - val_loss: 0.3975\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4085 - val_loss: 0.3960\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.3948\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.3936\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4039 - val_loss: 0.3921\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3907\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4010 - val_loss: 0.3894\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.3880\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.3870\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.3857\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3957 - val_loss: 0.3846\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.3834\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3931 - val_loss: 0.3825\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3921 - val_loss: 0.3813\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3906 - val_loss: 0.3808\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.3797\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.3785\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3776\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3864 - val_loss: 0.3770\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 0.3758\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.3748\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3739\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3826 - val_loss: 0.3731\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3725\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3714\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3707\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.3699\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3781 - val_loss: 0.3691\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3772 - val_loss: 0.3689\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3765 - val_loss: 0.3677\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3670\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3664\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3656\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3650\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3645\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3638\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.3630\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3705 - val_loss: 0.3624\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3617\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3610\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.3604\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.3599\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3668 - val_loss: 0.3600\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.3590\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.3583\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.3577\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.3572\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3573\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3635 - val_loss: 0.3561\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3628 - val_loss: 0.3558\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3624 - val_loss: 0.3554\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3618 - val_loss: 0.3550\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.3548\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.3540\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3540\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3596 - val_loss: 0.3530\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3903\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.8038 - val_loss: 1.1938\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0686 - val_loss: 0.7917\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7852 - val_loss: 0.6817\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7019 - val_loss: 0.6356\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6665 - val_loss: 0.6090\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6441 - val_loss: 0.5897\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6259 - val_loss: 0.5723\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.5577\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 0.5442\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5324\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5705 - val_loss: 0.5212\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 0.5117\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5494 - val_loss: 0.5025\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5401 - val_loss: 0.4941\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.4864\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5236 - val_loss: 0.4797\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4735\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5098 - val_loss: 0.4680\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5038 - val_loss: 0.4629\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4983 - val_loss: 0.4581\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4930 - val_loss: 0.4539\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4887 - val_loss: 0.4498\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4840 - val_loss: 0.4461\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4801 - val_loss: 0.4425\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4761 - val_loss: 0.4392\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4362\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4691 - val_loss: 0.4333\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 0.4309\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4283\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4599 - val_loss: 0.4258\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4570 - val_loss: 0.4233\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4541 - val_loss: 0.4208\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4515 - val_loss: 0.4186\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4490 - val_loss: 0.4165\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4465 - val_loss: 0.4151\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.4123\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4420 - val_loss: 0.4110\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4089\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4073\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4357 - val_loss: 0.4056\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4035\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4317 - val_loss: 0.4019\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4005\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4282 - val_loss: 0.3987\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.3972\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4246 - val_loss: 0.3956\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.3943\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.3931\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4199 - val_loss: 0.3916\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.3903\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.3893\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.3876\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.3863\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4126 - val_loss: 0.3853\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.3842\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4100 - val_loss: 0.3830\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.3819\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.3809\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 0.3800\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.3785\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4038 - val_loss: 0.3778\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.3770\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4015 - val_loss: 0.3760\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4004 - val_loss: 0.3759\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3741\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3731\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.3722\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3964 - val_loss: 0.3712\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3954 - val_loss: 0.3703\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3944 - val_loss: 0.3695\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3934 - val_loss: 0.3691\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.3683\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.3674\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3907 - val_loss: 0.3665\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.3653\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3889 - val_loss: 0.3647\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3880 - val_loss: 0.3640\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3870 - val_loss: 0.3635\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3863 - val_loss: 0.3627\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3855 - val_loss: 0.3618\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3846 - val_loss: 0.3615\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3607\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3831 - val_loss: 0.3598\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3591\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3814 - val_loss: 0.3590\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3577\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3801 - val_loss: 0.3572\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.3565\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3562\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3779 - val_loss: 0.3554\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3771 - val_loss: 0.3547\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3763 - val_loss: 0.3544\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3758 - val_loss: 0.3538\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3533\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3742 - val_loss: 0.3527\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3740 - val_loss: 0.3521\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3732 - val_loss: 0.3517\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3511\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3505\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3713 - val_loss: 0.3502\n",
      "121/121 [==============================] - 0s 981us/step - loss: 0.3647\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.4270 - val_loss: 1.2293\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0004 - val_loss: 0.7939\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7661 - val_loss: 0.6885\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6937 - val_loss: 0.6444\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6593 - val_loss: 0.6189\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6366 - val_loss: 0.5984\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6168 - val_loss: 0.5811\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 0.5652\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5518\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5392\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5593 - val_loss: 0.5291\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5483 - val_loss: 0.5191\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5089\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5010\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5211 - val_loss: 0.4932\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5132 - val_loss: 0.4865\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5064 - val_loss: 0.4798\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5003 - val_loss: 0.4742\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4943 - val_loss: 0.4691\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4893 - val_loss: 0.4647\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4842 - val_loss: 0.4604\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4800 - val_loss: 0.4561\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4756 - val_loss: 0.4527\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4719 - val_loss: 0.4488\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4681 - val_loss: 0.4454\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4429\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4615 - val_loss: 0.4393\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4588 - val_loss: 0.4367\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4558 - val_loss: 0.4346\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4536 - val_loss: 0.4322\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4510 - val_loss: 0.4303\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4484 - val_loss: 0.4273\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4462 - val_loss: 0.4252\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4440 - val_loss: 0.4236\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4216\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4402 - val_loss: 0.4199\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4176\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4362 - val_loss: 0.4160\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4345 - val_loss: 0.4149\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.4129\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4111\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4098\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4088\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4264 - val_loss: 0.4068\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.4061\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.4050\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4220 - val_loss: 0.4030\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4021\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4192 - val_loss: 0.3997\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.3984\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4167 - val_loss: 0.3971\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3967\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4140 - val_loss: 0.3958\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4129 - val_loss: 0.3934\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3924\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 0.3913\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3903\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.3893\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4071 - val_loss: 0.3882\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4060 - val_loss: 0.3871\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.3860\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.3860\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4028 - val_loss: 0.3837\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4013 - val_loss: 0.3842\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4005 - val_loss: 0.3816\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3995 - val_loss: 0.3817\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3797\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.3789\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.3780\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.3772\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3945 - val_loss: 0.3762\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3757\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3741\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 0.3733\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3909 - val_loss: 0.3735\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3898 - val_loss: 0.3721\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3710\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3883 - val_loss: 0.3702\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.3691\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3868 - val_loss: 0.3685\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 0.3682\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.3669\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.3660\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3835 - val_loss: 0.3658\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3830 - val_loss: 0.3650\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3820 - val_loss: 0.3647\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3640\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3806 - val_loss: 0.3627\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.3621\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3790 - val_loss: 0.3617\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.3610\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3618\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3770 - val_loss: 0.3595\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.3587\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3581\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3577\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.3571\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.3561\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3556\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3549\n",
      "121/121 [==============================] - 0s 917us/step - loss: 0.3825\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.0241 - val_loss: 2.7309\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0326 - val_loss: 1.3481\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1631 - val_loss: 0.9135\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8783 - val_loss: 0.7630\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7749 - val_loss: 0.7030\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7292 - val_loss: 0.6722\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7038 - val_loss: 0.6528\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6856 - val_loss: 0.6373\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6711 - val_loss: 0.6248\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6581 - val_loss: 0.6131\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6469 - val_loss: 0.6029\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6367 - val_loss: 0.5938\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6276 - val_loss: 0.5854\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6193 - val_loss: 0.5779\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6114 - val_loss: 0.5709\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6045 - val_loss: 0.5648\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5589\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5921 - val_loss: 0.5536\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5489\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5446\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5773 - val_loss: 0.5406\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5731 - val_loss: 0.5370\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5694 - val_loss: 0.5335\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5662 - val_loss: 0.5306\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 0.5278\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5252\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5575 - val_loss: 0.5229\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5554 - val_loss: 0.5207\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5535 - val_loss: 0.5189\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.5174\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.5156\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5476 - val_loss: 0.5145\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 0.5127\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5447 - val_loss: 0.5115\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5437 - val_loss: 0.5104\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5427 - val_loss: 0.5092\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5414 - val_loss: 0.5086\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5406 - val_loss: 0.5075\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5401 - val_loss: 0.5073\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5059\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.5053\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5367 - val_loss: 0.5057\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5373 - val_loss: 0.5049\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5366 - val_loss: 0.5043\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5353 - val_loss: 0.5033\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5032\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5029\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5342 - val_loss: 0.5031\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5017\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5014\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5336 - val_loss: 0.5021\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5336 - val_loss: 0.5019\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5327 - val_loss: 0.5007\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5335 - val_loss: 0.5008\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5327 - val_loss: 0.5014\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5320 - val_loss: 0.5003\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.5014\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5321 - val_loss: 0.5000\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.4997\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5323 - val_loss: 0.4996\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5008\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.5011\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.4999\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4995\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5007\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.5010\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4993\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.5001\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4997\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4992\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5002\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.5000\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4999\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.4991\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4995\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.5000\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5311 - val_loss: 0.4990\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4990\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5003\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.5003\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4990\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.4993\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4988\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4999\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.4991\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5002\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.4989\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.4996\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5007\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.5006\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5313 - val_loss: 0.4999\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5307 - val_loss: 0.5002\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5310 - val_loss: 0.4998\n",
      "121/121 [==============================] - 0s 750us/step - loss: 0.5435\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.7121 - val_loss: 2.6053\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9817 - val_loss: 1.2837\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1335 - val_loss: 0.8631\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8588 - val_loss: 0.7219\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7607 - val_loss: 0.6679\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7206 - val_loss: 0.6426\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6982 - val_loss: 0.6271\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6830 - val_loss: 0.6146\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6712 - val_loss: 0.6046\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6602 - val_loss: 0.5952\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6504 - val_loss: 0.5865\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6419 - val_loss: 0.5792\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6339 - val_loss: 0.5722\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6265 - val_loss: 0.5656\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6198 - val_loss: 0.5596\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6139 - val_loss: 0.5543\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6084 - val_loss: 0.5496\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6032 - val_loss: 0.5452\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.5409\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5944 - val_loss: 0.5373\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5902 - val_loss: 0.5340\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5308\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5836 - val_loss: 0.5280\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5808 - val_loss: 0.5255\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5781 - val_loss: 0.5231\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5756 - val_loss: 0.5209\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5732 - val_loss: 0.5189\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5713 - val_loss: 0.5172\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5690 - val_loss: 0.5152\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.5140\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5659 - val_loss: 0.5122\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5645 - val_loss: 0.5109\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5635 - val_loss: 0.5100\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5623 - val_loss: 0.5088\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5611 - val_loss: 0.5083\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5606 - val_loss: 0.5078\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5590 - val_loss: 0.5067\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5583 - val_loss: 0.5064\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5580 - val_loss: 0.5056\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5565 - val_loss: 0.5043\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 0.5041\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5558 - val_loss: 0.5039\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5553 - val_loss: 0.5028\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5548 - val_loss: 0.5030\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5545 - val_loss: 0.5021\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5538 - val_loss: 0.5015\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5540 - val_loss: 0.5012\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5530 - val_loss: 0.5016\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5533 - val_loss: 0.5015\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5529 - val_loss: 0.5014\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5529 - val_loss: 0.5006\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5522 - val_loss: 0.5012\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5526 - val_loss: 0.5010\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5523 - val_loss: 0.5000\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5520 - val_loss: 0.4999\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.4991\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5510 - val_loss: 0.5001\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.4989\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5513 - val_loss: 0.4997\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5508 - val_loss: 0.4987\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.4998\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5512 - val_loss: 0.4988\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.4982\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.4990\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.4985\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.4991\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5494 - val_loss: 0.4981\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5508 - val_loss: 0.4990\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.4983\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4983\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.4987\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.4980\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.4979\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5511 - val_loss: 0.4984\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5502 - val_loss: 0.4992\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4982\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5499 - val_loss: 0.4978\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.4993\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5496 - val_loss: 0.4982\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.4979\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.4986\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.4985\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.4994\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.4979\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5509 - val_loss: 0.4980\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.4976\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.4988\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.4979\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5504 - val_loss: 0.4988\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.4980\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4978\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4976\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5502 - val_loss: 0.4976\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.4975\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5510 - val_loss: 0.4978\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5502 - val_loss: 0.4992\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.4997\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.4979\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.4977\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.4995\n",
      "121/121 [==============================] - 0s 784us/step - loss: 0.5034\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.3942 - val_loss: 2.5796\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8381 - val_loss: 1.2963\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0995 - val_loss: 0.9043\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8645 - val_loss: 0.7705\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7802 - val_loss: 0.7173\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7424 - val_loss: 0.6896\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7200 - val_loss: 0.6710\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7031 - val_loss: 0.6557\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6887 - val_loss: 0.6426\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6759 - val_loss: 0.6306\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6640 - val_loss: 0.6200\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.6102\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6430 - val_loss: 0.6005\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6340 - val_loss: 0.5924\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6254 - val_loss: 0.5847\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6175 - val_loss: 0.5777\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6101 - val_loss: 0.5712\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6034 - val_loss: 0.5655\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.5597\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5546\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.5499\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5452\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5416\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5724 - val_loss: 0.5378\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5684 - val_loss: 0.5342\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5650 - val_loss: 0.5313\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5616 - val_loss: 0.5286\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5585 - val_loss: 0.5261\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5557 - val_loss: 0.5236\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5530 - val_loss: 0.5213\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5507 - val_loss: 0.5196\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5484 - val_loss: 0.5179\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 0.5160\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5443 - val_loss: 0.5143\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.5131\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5410 - val_loss: 0.5119\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5394 - val_loss: 0.5106\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5095\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5366 - val_loss: 0.5084\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5077\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5067\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5063\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.5051\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5046\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5041\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5298 - val_loss: 0.5037\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5033\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5285 - val_loss: 0.5028\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5023\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5273 - val_loss: 0.5020\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5017\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.5016\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5259 - val_loss: 0.5011\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.5008\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5251 - val_loss: 0.5008\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.5003\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5244 - val_loss: 0.5003\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5240 - val_loss: 0.5001\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5238 - val_loss: 0.4999\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 0.5000\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5232 - val_loss: 0.4997\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5230 - val_loss: 0.4997\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 0.4997\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.4995\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5224 - val_loss: 0.4995\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4995\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.4993\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.4994\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5218 - val_loss: 0.4993\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5217 - val_loss: 0.4994\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5215 - val_loss: 0.4991\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.4991\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5214 - val_loss: 0.4992\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 0.4992\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5212 - val_loss: 0.4995\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5211 - val_loss: 0.4994\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5210 - val_loss: 0.4995\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.4993\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5208 - val_loss: 0.4994\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.4994\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5207 - val_loss: 0.4995\n",
      "121/121 [==============================] - 0s 817us/step - loss: 0.5562\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8128 - val_loss: 0.6545\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8880 - val_loss: 0.7111\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7355 - val_loss: 0.5962\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8657 - val_loss: 0.6323\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7078 - val_loss: 0.5601\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7801 - val_loss: 0.6467\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7070 - val_loss: 0.5796\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7927 - val_loss: 0.6895\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8538 - val_loss: 0.5951\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9357 - val_loss: 0.6729\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8102 - val_loss: 0.6016\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8406 - val_loss: 0.7460\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8609 - val_loss: 0.6148\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8938 - val_loss: 0.7509\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9226 - val_loss: 0.6313\n",
      "121/121 [==============================] - 0s 708us/step - loss: 1.6230\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7830 - val_loss: 0.8615\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.9855 - val_loss: 1.7166\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.2071 - val_loss: 3.7049\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12.3251 - val_loss: 12.6017\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 44.1003 - val_loss: 41.5455\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 134.4400 - val_loss: 149.3797\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 396.5681 - val_loss: 533.9433\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1696.4774 - val_loss: 1845.4508\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6296.2305 - val_loss: 6441.6318\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 24621.3340 - val_loss: 22702.8379\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 71488.6484 - val_loss: 80272.7031\n",
      "121/121 [==============================] - 0s 924us/step - loss: 379995.3125\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8953 - val_loss: 1.9642\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 9.9445 - val_loss: 8.1881\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 49.9585 - val_loss: 41.9142\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 304.4659 - val_loss: 223.2838\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1443.4674 - val_loss: 1202.7617\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 15696.3232 - val_loss: 6661.9541\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 39121.6367 - val_loss: 35146.3906\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 238742.5312 - val_loss: 186742.5000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2375980.7500 - val_loss: 1013597.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6963116.5000 - val_loss: 5360026.5000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 64936528.0000 - val_loss: 30693418.0000\n",
      "121/121 [==============================] - 0s 778us/step - loss: 7725934.5000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.1128 - val_loss: 0.7982\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7530 - val_loss: 0.6575\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6794 - val_loss: 0.6089\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6311 - val_loss: 0.5702\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5970 - val_loss: 0.5440\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5671 - val_loss: 0.5203\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5465 - val_loss: 0.5020\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.4873\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5148 - val_loss: 0.4755\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.4656\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4968 - val_loss: 0.4595\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4824 - val_loss: 0.4595\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.4459\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4705 - val_loss: 0.4483\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4370\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4316\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4584 - val_loss: 0.4285\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4615 - val_loss: 0.4254\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.4229\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4490 - val_loss: 0.4196\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4163\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4141\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4120\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4093\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4281 - val_loss: 0.4068\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4050\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4037\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.4014\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4009\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.3962\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3946\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3946\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.3922\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.3893\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.3920\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.3871\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.3850\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.3848\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.3826\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3809\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3986 - val_loss: 0.3804\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.3790\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.3772\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.3767\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.3762\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3912 - val_loss: 0.3742\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3733\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.3720\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.3708\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3864 - val_loss: 0.3698\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.3685\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.3669\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.3658\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.3658\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3645\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3672\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3800 - val_loss: 0.3642\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3620\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3613\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3771 - val_loss: 0.3596\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 0.3602\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.3591\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3582\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.3582\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3569\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3555\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3551\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3548\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3544\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3688 - val_loss: 0.3545\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3536\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3522\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3517\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.3505\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.3498\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3646 - val_loss: 0.3506\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.3490\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3483\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3494\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.3475\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3472\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3622 - val_loss: 0.3473\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3464\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3458\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3457\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.3452\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3442\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 0.3437\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3442\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3429\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.3445\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3569 - val_loss: 0.3416\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3424\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 0.3414\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3553 - val_loss: 0.3414\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3403\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3552 - val_loss: 0.3400\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3401\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3392\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.3390\n",
      "121/121 [==============================] - 0s 917us/step - loss: 0.3849\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8715 - val_loss: 0.7475\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7476 - val_loss: 0.6696\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6904 - val_loss: 0.6243\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6484 - val_loss: 0.5889\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6169 - val_loss: 0.5592\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5888 - val_loss: 0.5361\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5675 - val_loss: 0.5169\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5012\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5357 - val_loss: 0.4907\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.4790\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.4715\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5032 - val_loss: 0.4649\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4961 - val_loss: 0.4588\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4893 - val_loss: 0.4575\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4822 - val_loss: 0.4467\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4787 - val_loss: 0.4496\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4726 - val_loss: 0.4377\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4708 - val_loss: 0.4342\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4657 - val_loss: 0.4310\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4622 - val_loss: 0.4284\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4289\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4568 - val_loss: 0.4220\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4527 - val_loss: 0.4223\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4491 - val_loss: 0.4190\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4153\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4115\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4118\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4134\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4071\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4073\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4022\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4000\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.3981\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.3971\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3944\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4225 - val_loss: 0.3955\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.3930\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.3919\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4164 - val_loss: 0.3883\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.3869\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.3855\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.3848\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3848\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.3818\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.3825\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3796\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.3781\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3775\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.3758\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.3751\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3739\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3729\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3986 - val_loss: 0.3717\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3712\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.3699\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3708\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.3686\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.3688\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.3682\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.3658\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.3651\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.3636\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.3643\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.3645\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3618\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3623\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.3612\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.3613\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3596\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3589\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3581\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3832 - val_loss: 0.3579\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3826 - val_loss: 0.3572\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3566\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.3557\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.3558\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3561\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3546\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3572\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.3541\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3531\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3562\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3520\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3518\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3515\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.3513\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3501\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3499\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3496\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3522\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3722 - val_loss: 0.3486\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3476\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3696 - val_loss: 0.3470\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3489\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3481\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.3474\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3467\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3464\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.3459\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3472\n",
      "121/121 [==============================] - 0s 785us/step - loss: 0.3721\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.8227 - val_loss: 0.7737\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7397 - val_loss: 0.6480\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6510 - val_loss: 0.6052\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6189 - val_loss: 0.5782\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5934 - val_loss: 0.5537\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5728 - val_loss: 0.5352\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5565 - val_loss: 0.5206\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5421 - val_loss: 0.5055\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5284 - val_loss: 0.4968\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5183 - val_loss: 0.4870\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5075 - val_loss: 0.4765\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5004 - val_loss: 0.4718\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4927 - val_loss: 0.4636\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4885 - val_loss: 0.4577\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4826 - val_loss: 0.4546\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4764 - val_loss: 0.4490\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4714 - val_loss: 0.4458\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4665 - val_loss: 0.4419\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4628 - val_loss: 0.4374\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4604 - val_loss: 0.4340\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4555 - val_loss: 0.4315\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.4276\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4250\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4221\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4456 - val_loss: 0.4201\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4165\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 0.4150\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4121\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4096\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4091\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4076\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4035\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4278 - val_loss: 0.4015\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4281 - val_loss: 0.3997\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3988\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.3991\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4187 - val_loss: 0.3962\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4178 - val_loss: 0.3948\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.3932\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3921\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3906\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4103 - val_loss: 0.3878\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.3871\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4079 - val_loss: 0.3852\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.3851\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.3822\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.3807\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4026 - val_loss: 0.3798\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4004 - val_loss: 0.3800\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3790\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3982 - val_loss: 0.3765\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.3749\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3771\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3965 - val_loss: 0.3727\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.3728\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3705\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3703\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3918 - val_loss: 0.3713\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3897 - val_loss: 0.3706\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.3691\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.3683\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.3652\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.3660\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.3649\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.3642\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3855 - val_loss: 0.3647\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.3621\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3612\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.3597\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3606\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3612\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3599\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3567\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3580\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3774 - val_loss: 0.3574\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3556\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3758 - val_loss: 0.3555\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3572\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3743 - val_loss: 0.3538\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.3537\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3738 - val_loss: 0.3537\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3743 - val_loss: 0.3533\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3527\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3507\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3509\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3500\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.3498\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3502\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3515\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3707 - val_loss: 0.3475\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3473\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3469\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.3454\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3467\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3457\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3465\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3446\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.3437\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3443\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3428\n",
      "121/121 [==============================] - 0s 888us/step - loss: 0.3681\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9761 - val_loss: 0.7266\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6899 - val_loss: 0.6004\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6180 - val_loss: 0.5579\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5776 - val_loss: 0.5277\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5025\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5228 - val_loss: 0.4844\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5041 - val_loss: 0.4670\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4876 - val_loss: 0.4532\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4731 - val_loss: 0.4428\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4600 - val_loss: 0.4311\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4220\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4437 - val_loss: 0.4150\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4331 - val_loss: 0.4085\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4251 - val_loss: 0.4028\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.3971\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4123 - val_loss: 0.3923\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.3890\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4031 - val_loss: 0.3882\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3807\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3771\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.3732\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3863 - val_loss: 0.3702\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3673\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3666\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.3622\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.3597\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3579\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3564\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3544\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3519\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3520\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3595 - val_loss: 0.3498\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3586 - val_loss: 0.3476\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3464\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3446\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.3429\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3506 - val_loss: 0.3426\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3420\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3477 - val_loss: 0.3398\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3462 - val_loss: 0.3389\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3442 - val_loss: 0.3374\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3386\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3416 - val_loss: 0.3355\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3404 - val_loss: 0.3329\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3323\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3374 - val_loss: 0.3315\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3361 - val_loss: 0.3308\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3348 - val_loss: 0.3289\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3337 - val_loss: 0.3289\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3332 - val_loss: 0.3283\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3313 - val_loss: 0.3287\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3304 - val_loss: 0.3253\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3263\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3229\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.3219\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3257 - val_loss: 0.3226\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.3213\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3244 - val_loss: 0.3209\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3231 - val_loss: 0.3195\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3189\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3221 - val_loss: 0.3184\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3189\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3177\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3184 - val_loss: 0.3164\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3183 - val_loss: 0.3155\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 0.3159\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3165 - val_loss: 0.3150\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.3147\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3163 - val_loss: 0.3132\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 0.3144\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3141 - val_loss: 0.3121\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3124 - val_loss: 0.3126\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.3141\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.3103\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3108 - val_loss: 0.3111\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3105 - val_loss: 0.3109\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3101 - val_loss: 0.3089\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3090\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 0.3081\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3065\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3059 - val_loss: 0.3090\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3046 - val_loss: 0.3087\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3045 - val_loss: 0.3122\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3040 - val_loss: 0.3047\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3024 - val_loss: 0.3054\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3014 - val_loss: 0.3046\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3011 - val_loss: 0.3036\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3030\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2997 - val_loss: 0.3027\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2992 - val_loss: 0.3050\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2987 - val_loss: 0.3011\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2981 - val_loss: 0.3010\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2970 - val_loss: 0.3030\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2971 - val_loss: 0.2999\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2960 - val_loss: 0.3005\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2972 - val_loss: 0.3015\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.2995\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2952 - val_loss: 0.2973\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2940 - val_loss: 0.2994\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2943 - val_loss: 0.2985\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3315\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.7702 - val_loss: 0.8308\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7119 - val_loss: 0.6143\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6331 - val_loss: 0.5654\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5297\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5599 - val_loss: 0.5053\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5344 - val_loss: 0.4841\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5138 - val_loss: 0.4674\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4963 - val_loss: 0.4543\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.4441\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4696 - val_loss: 0.4327\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4589 - val_loss: 0.4252\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4498 - val_loss: 0.4172\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.4107\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.4041\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.3994\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4203 - val_loss: 0.3925\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4139 - val_loss: 0.3946\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4092 - val_loss: 0.3841\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4038 - val_loss: 0.3797\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3997 - val_loss: 0.3764\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.3762\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3919 - val_loss: 0.3720\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3885 - val_loss: 0.3680\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3853 - val_loss: 0.3663\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3634\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3796 - val_loss: 0.3614\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3776 - val_loss: 0.3584\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3746 - val_loss: 0.3572\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3724 - val_loss: 0.3546\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3704 - val_loss: 0.3524\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3684 - val_loss: 0.3510\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3663 - val_loss: 0.3494\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3644 - val_loss: 0.3482\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3626 - val_loss: 0.3474\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3611 - val_loss: 0.3462\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3453\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.3430\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3423\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3549 - val_loss: 0.3426\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3535 - val_loss: 0.3407\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3522 - val_loss: 0.3394\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3509 - val_loss: 0.3391\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3375\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3362\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3473 - val_loss: 0.3348\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.3326\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3441 - val_loss: 0.3339\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3431 - val_loss: 0.3336\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3422 - val_loss: 0.3330\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3415 - val_loss: 0.3336\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3398 - val_loss: 0.3291\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3391 - val_loss: 0.3273\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3380 - val_loss: 0.3279\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3371 - val_loss: 0.3261\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.3262\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3355 - val_loss: 0.3249\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3343 - val_loss: 0.3246\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3339 - val_loss: 0.3232\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3328 - val_loss: 0.3221\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3322 - val_loss: 0.3234\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3224\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3202\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.3197\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3291 - val_loss: 0.3195\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3191\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3268 - val_loss: 0.3189\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.3187\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3167\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3185\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.3177\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3236 - val_loss: 0.3186\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3230 - val_loss: 0.3161\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3225 - val_loss: 0.3153\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3218 - val_loss: 0.3136\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3132\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3203 - val_loss: 0.3126\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3202 - val_loss: 0.3127\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3191 - val_loss: 0.3129\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.3106\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3174 - val_loss: 0.3115\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3108\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3163 - val_loss: 0.3093\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3156 - val_loss: 0.3085\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3161\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3147 - val_loss: 0.3112\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3073\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3129 - val_loss: 0.3104\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3144 - val_loss: 0.3065\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3095\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3152 - val_loss: 0.3066\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 0.3194\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3174 - val_loss: 0.3067\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3179 - val_loss: 0.3135\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.3053\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3183 - val_loss: 0.3087\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3141 - val_loss: 0.3048\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3155 - val_loss: 0.3098\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3117 - val_loss: 0.3034\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.3067\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3105 - val_loss: 0.3065\n",
      "121/121 [==============================] - 0s 883us/step - loss: 0.3211\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.7328 - val_loss: 0.7712\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7145 - val_loss: 0.6356\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6410 - val_loss: 0.5893\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 0.5559\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 0.5300\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 0.5094\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.4895\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5069 - val_loss: 0.4764\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4916 - val_loss: 0.4605\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4784 - val_loss: 0.4494\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4668 - val_loss: 0.4384\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4567 - val_loss: 0.4308\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4476 - val_loss: 0.4240\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4397 - val_loss: 0.4153\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.4114\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4270 - val_loss: 0.4042\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4216 - val_loss: 0.4014\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4162 - val_loss: 0.3953\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.3915\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4065 - val_loss: 0.3872\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.3825\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3995 - val_loss: 0.3796\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3951 - val_loss: 0.3784\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3926 - val_loss: 0.3743\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3892 - val_loss: 0.3714\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3865 - val_loss: 0.3685\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3842 - val_loss: 0.3655\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3630\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.3611\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3770 - val_loss: 0.3607\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3745 - val_loss: 0.3568\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3567\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3697 - val_loss: 0.3543\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3552\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3662 - val_loss: 0.3489\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3470\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3463\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.3470\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3590 - val_loss: 0.3432\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3575 - val_loss: 0.3427\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3550 - val_loss: 0.3410\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3542 - val_loss: 0.3397\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3513 - val_loss: 0.3368\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3503 - val_loss: 0.3355\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3485 - val_loss: 0.3357\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3474 - val_loss: 0.3341\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3463 - val_loss: 0.3326\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3318\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3303\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3421 - val_loss: 0.3303\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3397 - val_loss: 0.3290\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3384 - val_loss: 0.3258\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.3255\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3350 - val_loss: 0.3250\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3349 - val_loss: 0.3235\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3329 - val_loss: 0.3236\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3308 - val_loss: 0.3234\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3316 - val_loss: 0.3218\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3292 - val_loss: 0.3210\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3180\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3266 - val_loss: 0.3191\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.3208\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.3158\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3242 - val_loss: 0.3166\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3231 - val_loss: 0.3160\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3227 - val_loss: 0.3153\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3198 - val_loss: 0.3162\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3202 - val_loss: 0.3125\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3191 - val_loss: 0.3156\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3102\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3167 - val_loss: 0.3159\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3159 - val_loss: 0.3108\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3152 - val_loss: 0.3090\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3133 - val_loss: 0.3097\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3131 - val_loss: 0.3086\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3117 - val_loss: 0.3084\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3119 - val_loss: 0.3068\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3096 - val_loss: 0.3074\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3045\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3071 - val_loss: 0.3068\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3070 - val_loss: 0.3063\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3058 - val_loss: 0.3034\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3049 - val_loss: 0.3072\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3063 - val_loss: 0.3069\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3033\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3036 - val_loss: 0.3010\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3020 - val_loss: 0.3026\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3019 - val_loss: 0.3007\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3001 - val_loss: 0.3007\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3003 - val_loss: 0.2999\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2997 - val_loss: 0.3001\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2988\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2979 - val_loss: 0.3013\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2974 - val_loss: 0.2992\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 0.2986\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2948 - val_loss: 0.2984\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2942 - val_loss: 0.2970\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2937 - val_loss: 0.2959\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2930 - val_loss: 0.3003\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2927 - val_loss: 0.2961\n",
      "121/121 [==============================] - 0s 1000us/step - loss: 0.3156\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9703 - val_loss: 0.6302\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7818 - val_loss: 0.5772\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6428 - val_loss: 0.4589\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4783 - val_loss: 0.4726\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4333\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4219\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4181\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4007\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.3927\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.3878\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4034 - val_loss: 0.3814\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4110 - val_loss: 0.3803\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3751\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.3713\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3918\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3887 - val_loss: 0.3657\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3621\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3608\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.3604\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3572\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3576\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3717 - val_loss: 0.3575\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3847 - val_loss: 0.3568\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3521\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.3666\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3519\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.3524\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3468\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.3464\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3442\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3439\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3578 - val_loss: 0.3457\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3417\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3559 - val_loss: 0.3401\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4023 - val_loss: 0.4325\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3541\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3610 - val_loss: 0.3377\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3367\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.3423\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3342\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3392\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3347\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3316\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3321\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.3383\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3322\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3326\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.3321\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3437 - val_loss: 0.3324\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3275\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3302\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.3267\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3248\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.3300\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3371 - val_loss: 0.3368\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.3233\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3239\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3246\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3355 - val_loss: 0.3385\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.3260\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3337 - val_loss: 0.3197\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3189\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3198\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3342 - val_loss: 0.3187\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3225\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3293 - val_loss: 0.3249\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3458 - val_loss: 0.3187\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.3234\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3294 - val_loss: 0.3264\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3195\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3384 - val_loss: 0.3159\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3173\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3156\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3333 - val_loss: 0.3172\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3159\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3188\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3180\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3261 - val_loss: 0.3146\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3260 - val_loss: 0.3223\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.3133\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3244 - val_loss: 0.3157\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3112\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3106\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3205 - val_loss: 0.3129\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3160\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3143\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3105\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3114\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3172\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3641 - val_loss: 0.3234\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3112\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3102\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3115\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.3088\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3182 - val_loss: 0.3109\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3174 - val_loss: 0.3085\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3188 - val_loss: 0.3086\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3075\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3112\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3149 - val_loss: 0.3085\n",
      "121/121 [==============================] - 0s 765us/step - loss: 0.3547\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1197 - val_loss: 0.6654\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0326 - val_loss: 0.7461\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9406 - val_loss: 0.5300\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5695 - val_loss: 0.4627\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.4339\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4596 - val_loss: 0.4190\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4446 - val_loss: 0.4141\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4113\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.3905\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3848\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4099 - val_loss: 0.3828\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.3798\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3722\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.3704\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.3663\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3683\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.3613\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.3610\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.3620\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3584\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3647\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.3549\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3548\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3554\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3522\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.3512\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3497\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3461\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.3516\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3453\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3629 - val_loss: 0.3445\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.3418\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3420\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3400\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 0.3398\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3583 - val_loss: 0.3419\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3409\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3384\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3367\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3593 - val_loss: 0.4121\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4890 - val_loss: 0.3574\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3636 - val_loss: 0.3412\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3364\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3573 - val_loss: 0.3352\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3357\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3366\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3357\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3325\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3324\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3327\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3340\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3298\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3309\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3282\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3283\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3310\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3259\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3297\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3254\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.3271\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3252\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3230\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.3266\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3251\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 0.3246\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3397 - val_loss: 0.3296\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3367 - val_loss: 0.3235\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3207\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3345 - val_loss: 0.3234\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3330 - val_loss: 0.3216\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3323 - val_loss: 0.3244\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3322 - val_loss: 0.3196\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3230\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3308 - val_loss: 0.3215\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3300 - val_loss: 0.3208\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3291 - val_loss: 0.3214\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3194\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3238\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3320 - val_loss: 0.3181\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3294 - val_loss: 0.3158\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3202\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3148\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.3144\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3261 - val_loss: 0.3191\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3261 - val_loss: 0.3181\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3152\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3155\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3132\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3123\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3239 - val_loss: 0.3185\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3138\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3252 - val_loss: 0.3124\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.3131\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3220 - val_loss: 0.3157\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3239\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3240 - val_loss: 0.3146\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3155\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3145\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3119\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3219 - val_loss: 0.3111\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3268\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.5007 - val_loss: 0.6272\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5748 - val_loss: 0.4856\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4898 - val_loss: 0.4460\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4209\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.4128\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4029\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4209 - val_loss: 0.3929\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.3817\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.3776\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4042 - val_loss: 0.3734\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.3729\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.3722\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3660\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.3612\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3605\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3867 - val_loss: 0.3637\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.3580\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3592\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3792 - val_loss: 0.3532\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.3525\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.3503\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3750 - val_loss: 0.3496\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3500\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3457\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.3492\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3467\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3652 - val_loss: 0.3446\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3439\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3617 - val_loss: 0.3416\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3404\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3397\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3588 - val_loss: 0.3384\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3441\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3554 - val_loss: 0.3419\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3543 - val_loss: 0.3382\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3536 - val_loss: 0.3375\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3350\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3375\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3361\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3319\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3355\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3416\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3455 - val_loss: 0.3398\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3331\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3437 - val_loss: 0.3304\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3434 - val_loss: 0.3291\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3262\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3321\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3259\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3332\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3408 - val_loss: 0.3262\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.3271\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3380 - val_loss: 0.3260\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3358 - val_loss: 0.3240\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3271\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3245\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3205\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3192\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3228\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3222\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3176\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3304 - val_loss: 0.3166\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.3191\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3172\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3160\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3185\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3181\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3150\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3250 - val_loss: 0.3156\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3243 - val_loss: 0.3195\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3185\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3199\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3165\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3203\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3216 - val_loss: 0.3182\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3213\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3147\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3127\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3115\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3206 - val_loss: 0.3209\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3092\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3137\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3168 - val_loss: 0.3108\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.3113\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3110\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3080\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3094\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3147 - val_loss: 0.3088\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3096\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.3080\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3135 - val_loss: 0.3196\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3187 - val_loss: 0.3135\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.3152\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3063\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3137 - val_loss: 0.3070\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.3066\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3090\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.3087\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3089\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3130 - val_loss: 0.3028\n",
      "121/121 [==============================] - 0s 933us/step - loss: 0.3279\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2563 - val_loss: 1.7412\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4914 - val_loss: 1.0523\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9994 - val_loss: 0.8091\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8057 - val_loss: 0.7057\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7257 - val_loss: 0.6613\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6875 - val_loss: 0.6347\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6649 - val_loss: 0.6179\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6489 - val_loss: 0.6051\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6361 - val_loss: 0.5942\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6251 - val_loss: 0.5849\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6149 - val_loss: 0.5766\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6062 - val_loss: 0.5689\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5619\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.5550\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5488\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5763 - val_loss: 0.5429\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5700 - val_loss: 0.5376\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5639 - val_loss: 0.5326\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5584 - val_loss: 0.5274\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5532 - val_loss: 0.5229\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5481 - val_loss: 0.5186\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 0.5142\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5385 - val_loss: 0.5105\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5068\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.5030\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5263 - val_loss: 0.4996\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5224 - val_loss: 0.4963\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4933\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5154 - val_loss: 0.4903\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.4875\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.4850\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.4824\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5032 - val_loss: 0.4798\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5005 - val_loss: 0.4774\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.4749\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4954 - val_loss: 0.4729\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4928 - val_loss: 0.4710\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4905 - val_loss: 0.4686\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.4666\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.4649\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4838 - val_loss: 0.4632\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.4615\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4798 - val_loss: 0.4593\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4575\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4759 - val_loss: 0.4558\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.4544\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4719 - val_loss: 0.4524\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4508\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4684 - val_loss: 0.4494\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4668 - val_loss: 0.4479\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4464\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.4450\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4616 - val_loss: 0.4438\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4598 - val_loss: 0.4423\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.4411\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4567 - val_loss: 0.4393\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4550 - val_loss: 0.4386\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.4368\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4353\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4506 - val_loss: 0.4341\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4492 - val_loss: 0.4329\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4320\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4306\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4295\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4279\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4420 - val_loss: 0.4267\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4257\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4242\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4230\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4362 - val_loss: 0.4221\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4208\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4195\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4185\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4177\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.4164\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4155\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4144\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4135\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4124\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4243 - val_loss: 0.4115\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4107\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.4096\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4086\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4199 - val_loss: 0.4077\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4067\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4060\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4050\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.4046\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4034\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4140 - val_loss: 0.4027\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4021\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.4008\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.4001\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4104 - val_loss: 0.3992\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.3986\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.3977\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.3968\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.3961\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4060 - val_loss: 0.3955\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4051 - val_loss: 0.3946\n",
      "121/121 [==============================] - 0s 917us/step - loss: 0.4349\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.1184 - val_loss: 1.8490\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5871 - val_loss: 1.2696\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2082 - val_loss: 1.0440\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0245 - val_loss: 0.9119\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9149 - val_loss: 0.8269\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8467 - val_loss: 0.7728\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8024 - val_loss: 0.7364\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7719 - val_loss: 0.7105\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7492 - val_loss: 0.6902\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7312 - val_loss: 0.6739\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7158 - val_loss: 0.6597\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7020 - val_loss: 0.6466\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6893 - val_loss: 0.6347\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6774 - val_loss: 0.6233\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6659 - val_loss: 0.6125\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6549 - val_loss: 0.6022\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6443 - val_loss: 0.5923\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6341 - val_loss: 0.5827\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6239 - val_loss: 0.5734\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6143 - val_loss: 0.5645\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6048 - val_loss: 0.5561\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5958 - val_loss: 0.5477\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5401\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5323\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5702 - val_loss: 0.5247\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5624 - val_loss: 0.5180\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5549 - val_loss: 0.5114\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.5052\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5409 - val_loss: 0.4991\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5345 - val_loss: 0.4934\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5285 - val_loss: 0.4882\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5229 - val_loss: 0.4831\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5170 - val_loss: 0.4785\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5132 - val_loss: 0.4740\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5088 - val_loss: 0.4703\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 0.4670\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5010 - val_loss: 0.4638\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4975 - val_loss: 0.4608\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.4576\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4910 - val_loss: 0.4545\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4883 - val_loss: 0.4519\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4855 - val_loss: 0.4503\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4830 - val_loss: 0.4481\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4803 - val_loss: 0.4460\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4776 - val_loss: 0.4429\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4755 - val_loss: 0.4411\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4737 - val_loss: 0.4396\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4372\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.4355\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4674 - val_loss: 0.4343\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.4337\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4318\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4296\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4599 - val_loss: 0.4286\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4585 - val_loss: 0.4273\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4567 - val_loss: 0.4260\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4550 - val_loss: 0.4238\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4227\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4225\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.4202\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4492 - val_loss: 0.4195\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4178\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4461 - val_loss: 0.4164\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4448 - val_loss: 0.4167\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4139\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 0.4146\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4124\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4114\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4111\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4097\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4083\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4071\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4071\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4056\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4047\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4049\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4034\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4019\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.4024\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4003\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.4002\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.3996\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3989\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4234 - val_loss: 0.3978\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.3965\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.3965\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.3952\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3949\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.3938\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4183 - val_loss: 0.3941\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3923\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.3913\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.3907\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4150 - val_loss: 0.3907\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.3902\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4136 - val_loss: 0.3897\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3889\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3879\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.3885\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.3869\n",
      "121/121 [==============================] - 0s 910us/step - loss: 0.3987\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.8126 - val_loss: 1.3413\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2848 - val_loss: 0.9020\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8893 - val_loss: 0.7597\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7476 - val_loss: 0.6781\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6889 - val_loss: 0.6422\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6586 - val_loss: 0.6204\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6399 - val_loss: 0.6056\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.5933\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.5835\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6039 - val_loss: 0.5746\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.5664\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5597\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5796 - val_loss: 0.5527\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5731 - val_loss: 0.5463\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5668 - val_loss: 0.5407\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5610 - val_loss: 0.5352\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5556 - val_loss: 0.5304\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.5255\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5219\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5412 - val_loss: 0.5171\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5131\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5328 - val_loss: 0.5094\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5057\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5024\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.4988\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.4959\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.4925\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5110 - val_loss: 0.4893\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5079 - val_loss: 0.4867\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.4837\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5016 - val_loss: 0.4818\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 0.4783\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4960 - val_loss: 0.4755\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4935 - val_loss: 0.4732\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4908 - val_loss: 0.4709\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4883 - val_loss: 0.4683\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4858 - val_loss: 0.4663\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4833 - val_loss: 0.4641\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4807 - val_loss: 0.4621\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4786 - val_loss: 0.4595\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4570\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.4549\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4716 - val_loss: 0.4526\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.4506\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4672 - val_loss: 0.4489\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4465\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4441\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4607 - val_loss: 0.4425\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4587 - val_loss: 0.4403\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4383\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4545 - val_loss: 0.4365\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4527 - val_loss: 0.4347\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4506 - val_loss: 0.4330\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4313\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.4298\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4288\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.4263\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4244\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4231\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4217\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4206\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4189\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4342 - val_loss: 0.4176\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4161\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4156\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4137\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4128\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4115\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.4107\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4095\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4088\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4085\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4072\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4059\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4052\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4053\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4033\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.4027\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.4021\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4008\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4001\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.3993\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.3987\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.3991\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.3983\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.3964\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.3959\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.3951\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.3942\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.3937\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.3936\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.3924\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.3917\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.3909\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.3913\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.3900\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.3894\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.3891\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.3881\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.3877\n",
      "121/121 [==============================] - 0s 863us/step - loss: 0.4158\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3825 - val_loss: 0.6583\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6508 - val_loss: 0.5764\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5892 - val_loss: 0.5275\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5474 - val_loss: 0.4943\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5162 - val_loss: 0.4689\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4935 - val_loss: 0.4501\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4752 - val_loss: 0.4345\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4234\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4110\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4361 - val_loss: 0.4022\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.3950\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.3897\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.3820\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.3769\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.3706\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3923 - val_loss: 0.3663\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3871 - val_loss: 0.3622\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3828 - val_loss: 0.3597\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.3559\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3530\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.3494\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3472\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3468\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3623 - val_loss: 0.3417\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3402\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3576 - val_loss: 0.3383\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3549 - val_loss: 0.3376\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3530 - val_loss: 0.3357\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3512 - val_loss: 0.3335\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3486 - val_loss: 0.3299\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3305\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3452 - val_loss: 0.3282\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3423 - val_loss: 0.3261\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3420 - val_loss: 0.3267\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3398 - val_loss: 0.3239\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.3241\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.3241\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3354 - val_loss: 0.3206\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3328 - val_loss: 0.3208\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3319 - val_loss: 0.3189\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.3178\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3294 - val_loss: 0.3174\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3282 - val_loss: 0.3154\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3168\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3247 - val_loss: 0.3145\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3233 - val_loss: 0.3139\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3234 - val_loss: 0.3127\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3211 - val_loss: 0.3129\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3200 - val_loss: 0.3175\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3205 - val_loss: 0.3100\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3186 - val_loss: 0.3100\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3170 - val_loss: 0.3100\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3177 - val_loss: 0.3087\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3169 - val_loss: 0.3085\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3153 - val_loss: 0.3097\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3138 - val_loss: 0.3076\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3127 - val_loss: 0.3075\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3116 - val_loss: 0.3053\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3114 - val_loss: 0.3041\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3098 - val_loss: 0.3032\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3091 - val_loss: 0.3048\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.3035\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3076 - val_loss: 0.3033\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3074 - val_loss: 0.3045\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3060 - val_loss: 0.3039\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3046 - val_loss: 0.3006\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3045 - val_loss: 0.3028\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3042 - val_loss: 0.3009\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3047 - val_loss: 0.3009\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3038 - val_loss: 0.3025\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3012 - val_loss: 0.2986\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.2987\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3015 - val_loss: 0.2975\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2996 - val_loss: 0.2964\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2981 - val_loss: 0.2973\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2984 - val_loss: 0.2954\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2984 - val_loss: 0.2967\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2966 - val_loss: 0.2962\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2957 - val_loss: 0.2942\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2970 - val_loss: 0.2947\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2952 - val_loss: 0.2937\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2943 - val_loss: 0.2931\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2939 - val_loss: 0.2968\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2927 - val_loss: 0.2945\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2934 - val_loss: 0.2951\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2942 - val_loss: 0.2924\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2919 - val_loss: 0.2933\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2904 - val_loss: 0.2920\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2905 - val_loss: 0.2909\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2906 - val_loss: 0.2907\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2912 - val_loss: 0.2939\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2888 - val_loss: 0.2931\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2889 - val_loss: 0.2917\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2885 - val_loss: 0.2888\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2889 - val_loss: 0.2887\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2877 - val_loss: 0.2882\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2882 - val_loss: 0.2898\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2860 - val_loss: 0.2939\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2856 - val_loss: 0.2897\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2842 - val_loss: 0.2886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x0000027B4FEF4610>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000027B4FEFF1F0>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이퍼파라미터가 많으므로 GridSearchCV보다 RandomizedSearchCV가 적절\n",
    "# 탐색할 매개변수: 은닉층 갯수, 뉴런 갯수, 학습률\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\":[0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1,100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                validation_data=(X_valid, y_valid), # 검증에 k-fold를 이용하므로 valdata는 사실 필요 없음. EarlyStopping에 사용하는 용도\n",
    "                callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001481133513058748, 'n_hidden': 3, 'n_neurons': 94}\n",
      "-0.32272225618362427\n"
     ]
    }
   ],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3.1 은닉층 개수\n",
    "- 은닉층이 하나여도 뉴런 개수가 충분하면 복잡한 함수도 모델링 가능\n",
    "- 은닉층이 많은 심층 신경망이 얕은 신경망보다 복잡한 문제에서는 파라미터 효율성이 좋음\n",
    "    - 더 적은 수의 뉴런을 사용하므로 동일한 양의 훈련 데이터에서 더 높은 성능을 낼 수 있음\n",
    "- 계층 구조는 심층 신경망이 좋은 솔루션으로 빨리 수렴하게끔 도와줄 뿐만 아니라 새로운 데이터에 일반화되는 능력도 향상시켜줌 -> 전이학습\n",
    "- 복잡한 작업(대규모 이미지 분류, 음성 인식 등)에서는 수십 개의 층으로 이루어진 네트워크가 필요함\n",
    "    - 비슷한 작업에서 가장 뛰어난 성능을 낸 미리 훈련된 네트워크 일부를 재사용하여 전이학습 시키는것이 일반적\n",
    "    - 훈련속도 빠르고 데이터도 적게 필요함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3.2 은닉층의 뉴런 개수\n",
    "- 입력층과 출력층의 뉴런 개수는 해당 작업에 필요한 입력과 출력의 형태에 따라 결정\n",
    "- 은닉층의 구성방식: 각 층의 뉴런을 점점 줄여서 깔때기처럼 구성. 저수준의 많은 특성이 고수준의 적은 특성으로 합쳐질 수 있기 때문\n",
    "- 스트레치 팬츠 방식: 맞는 사이즈의 바지를 찾는것보다 큰 바지를 사고 알맞게 줄이는게 효과적\n",
    "    - 점진적으로 뉴런의 수를 늘리는 것보다는, 필요한 것보다 더 많은 층과 뉴런을 가진 모델을 선택하고 과대 적합되지 않도록 조기 종료나 규제기법을 사용하는것이 더 간단하고 효과적\n",
    "    - 모델에서 문제를 일으키는 병목층을 피할 수 있음\n",
    "- 뉴런의 수가 너무 적을시 문제점: 입력에 있는 유용한 정보를 모두 유지하기 위한 충분한 표현능력을 가지지 못함. 정보 소실 가능성\n",
    "- 층의 뉴런수를 늘리는것보단 층 수를 늘리는게 더 나을수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3.3 학습률, 배치 크기 그리고 다른 하이퍼파라미터(11장 참조)\n",
    "1. 학습률: 최대 학습률의 절반 정도가 최적의 학습률\n",
    "2. 옵티마이저: 고전적인 미니배치 경사 하강법보다 더 좋은 옵티마이저 선택도 매우 중요\n",
    "3. 배치크기: 모델성능과 훈련시간에 큰 영향. GPU에 맞는 가장 큰 배치크기 사용 권장. 그러나 너무 큰 배치를 사용하면 훈련 초기 불안정하게 훈련되어 일반화 성능이 떨어지기도. 학습률 예열 등의 기법을 사용하면 큰 배치 크기도 일반화 성능에 영향 미치지 않음\n",
    "4. 활성화 함수: 은닉층은 ReLU가 좋은 기본값. 출력층은 수행하는 작업에 따라 달라짐 \n",
    "5. 반복 횟수: 튜닝할 필요 없음. 조기 종료를 사용"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b54ea5334ac7e30e14c15345e162e7c7584df602af4968ddc075c99bdced931e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('cakd3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
